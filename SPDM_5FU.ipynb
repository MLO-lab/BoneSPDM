{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol for Bone Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Libraries and Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import pickle\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy\n",
    "from PIL import Image\n",
    "from imaris_ims_file_reader.ims import ims\n",
    "from skimage import measure, morphology\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import Rbf\n",
    "from matplotlib.patches import Patch\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/fm07-admin/Projects/DisVis/')\n",
    "# Increase the maximum image size limit\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "data_dir_5fu = 'data_metabone_5fu'\n",
    "save_dir = '/'\n",
    "\n",
    "mk_color_map = {\n",
    "    'MKs': \"#FF00FF\",  # Magenta \n",
    "    'Adipos': \"#C96500\",  # Brown \n",
    "    'Sinusoids': \"#00FFFF\",  # Cyan # Marker CD105 = Sinusoids\n",
    "    'MSCs': '#FFCC00',  # Yellow # Marker Cxcl12 = MSCs\n",
    "    'Neurons': \"#FF7F00\",  # Orange  TODO: the value is missing\n",
    "    'Arteries': \"#33A02C\", # Green\n",
    "    'GFP': \"#A6CEE3\" # Light Blue\n",
    "}\n",
    "\n",
    "\n",
    "hscs_color_map = {\n",
    "    'HSCs': \"#A6CEE3\" ,  # Light Blue\n",
    "    'RDs' : \"#A9A9A9\" # Gray\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function to filter and get the Position.csv files by condition\n",
    "def get_position_files_by_conditions():\n",
    "    # Lists to store Position.csv files for each condition\n",
    "    position_files_d5 = []\n",
    "    position_files_d10 = []\n",
    "    position_files_d15 = []\n",
    "    position_files_d30 = []\n",
    "    position_files_ss = []\n",
    "    \n",
    "    # Find all directories in data_dir_5fu, excluding .ims files\n",
    "    all_dirs = [os.path.join(data_dir_5fu, d) for d in os.listdir(data_dir_5fu) if d.startswith('d')] # get all the directories starting with 2\n",
    "    \n",
    "    # Classify the directories into d5, d10, d15, d30, or \"ss\"\n",
    "    for data_dir in all_dirs:\n",
    "        \n",
    "        sub_dirs = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('Statistics')]\n",
    "        for sub_dir in sub_dirs:\n",
    "            # Get the first file ending with 'Position.csv' in each sub-directory\n",
    "            # First check for Position_Reference_Frame.csv, if not found, fallback to Position.csv\n",
    "            position_file = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('Position_Reference_Frame.csv')]\n",
    "            if not position_file:\n",
    "                # Only look for Position.csv if Position_Reference_Frame.csv is not found\n",
    "                position_file = [os.path.join(sub_dir, f) for f in os.listdir(sub_dir) if f.endswith('Position.csv')]\n",
    "        \n",
    "            if position_file:\n",
    "                if 'd5' in data_dir:\n",
    "                    position_files_d5.append(position_file[0])\n",
    "                elif 'd10' in data_dir:\n",
    "                    position_files_d10.append(position_file[0])\n",
    "                elif 'd15' in data_dir:\n",
    "                    position_files_d15.append(position_file[0])\n",
    "                elif 'd30' in data_dir:\n",
    "                    position_files_d30.append(position_file[0])\n",
    "                else:\n",
    "                    # Files that don't contain 'd5', 'd10', 'd15', or 'd30' are classified as \"ss\"\n",
    "                    position_files_ss.append(position_file[0])\n",
    "\n",
    "    # Return a dictionary with the results\n",
    "    return {\n",
    "        'd5': position_files_d5,\n",
    "        'd10': position_files_d10,\n",
    "        'd15': position_files_d15,\n",
    "        'd30': position_files_d30,\n",
    "        'ss': position_files_ss\n",
    "    }\n",
    "\n",
    "# Get position files for d5, d10, d15, d30, and the \"ss\" case\n",
    "position_files_by_conditions = get_position_files_by_conditions()\n",
    "# position_files_by_conditions_hsc = get_position_files_by_conditions_hsc()\n",
    "\n",
    "# Access individual lists for each condition\n",
    "position_dirs_d5 = position_files_by_conditions['d5']\n",
    "position_dirs_d10 = position_files_by_conditions['d10']\n",
    "position_dirs_d15 = position_files_by_conditions['d15']\n",
    "position_dirs_d30 = position_files_by_conditions['d30'] \n",
    "position_dirs_ss = position_files_by_conditions['ss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read the position files and create the DataFrames with columns 'Position.x', 'Position.y', 'Position.z'\n",
    "# Regular expression to capture the word that starts with 'm' and extract the part after 'm'\n",
    "pattern = r'\\bm([A-Za-z0-9]+)'\n",
    "\n",
    "def process_position_dirs(position_dirs, scale_factors):\n",
    "    \"\"\"\n",
    "    Process a list of CSV files containing position data, scales the coordinates,\n",
    "    and stores the resulting DataFrame in the provided output dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    position_dirs (list): List of file paths to CSV files containing the position data.\n",
    "    scale_factors (dict): Dictionary with scaling factors for 'x', 'y', and 'z'.\n",
    "                        e.g., {'x': 0.7575, 'y': 0.7575, 'z': 2.5}\n",
    "    Returns:\n",
    "    None: The function modifies the provided output_dict in place.\n",
    "    \"\"\"\n",
    "    output_dict = {}\n",
    "    for position_dir in position_dirs:\n",
    "        # print(position_dir.split('/')[-1])\n",
    "        df = pd.read_csv(position_dir, skiprows=3)\n",
    "        \n",
    "        # Take the first 3 columns as the position coordinates\n",
    "        df = df.iloc[:, :3]\n",
    "        df.columns = ['Position.X', 'Position.Y', 'Position.Z']\n",
    "        \n",
    "        # Scale the position coordinates\n",
    "        df['Position.X'] = df['Position.X'] / scale_factors['x']\n",
    "        df['Position.Y'] = df['Position.Y'] / scale_factors['y']\n",
    "        df['Position.Z'] = df['Position.Z'] / scale_factors['z']\n",
    "        \n",
    "        # Extract a key name for the output dictionary from the file path\n",
    "        match = re.search(pattern, position_dir)\n",
    "        if match:\n",
    "            df_name_prefix = '_'.join(position_dir.split('/')[-1].split(' ')[:2])\n",
    "            if match.group(1) == 'MK':\n",
    "                df_name = '_'.join([df_name_prefix, 'MKs'])\n",
    "            elif match.group(1) == 'Adipo':\n",
    "                df_name = '_'.join([df_name_prefix, 'Adipos'])\n",
    "            elif match.group(1) == 'Cxcl12':\n",
    "                df_name = '_'.join([df_name_prefix, 'MSCs'])\n",
    "            elif match.group(1) == 'CD105':\n",
    "                df_name = '_'.join([df_name_prefix, 'Sinusoids'])\n",
    "            else:\n",
    "                df_name = '_'.join([df_name_prefix, match.group(1)])\n",
    "            print(df_name)\n",
    "            \n",
    "        elif 'HSC' in position_dir:\n",
    "            df_name_prefix = '_'.join(position_dir.split('/')[-1].split(' ')[:2])\n",
    "            df_name = '_'.join([df_name_prefix, 'HSCs'])\n",
    "            print(df_name)\n",
    "            # df_name = position_dir.split('/')[-1].split('.')[0]\n",
    "        elif 'RD' in position_dir:\n",
    "            df_name_prefix = '_'.join(position_dir.split('/')[-1].split(' ')[:2])\n",
    "            df_name = '_'.join([df_name_prefix, 'RDs'])\n",
    "            print(df_name)\n",
    "        elif 'Sca1' in position_dir:\n",
    "            df_name_prefix = '_'.join(position_dir.split('/')[-1].split(' ')[:2])\n",
    "            df_name = '_'.join([df_name_prefix, 'Arteries'])\n",
    "            print(df_name)\n",
    "        elif 'GFAP' in position_dir:\n",
    "            df_name_prefix = '_'.join(position_dir.split('/')[-1].split(' ')[:2])\n",
    "            df_name = '_'.join([df_name_prefix, 'Neurons'])\n",
    "            print(df_name)\n",
    "        \n",
    "        # Store the processed DataFrame in the output dictionary\n",
    "        output_dict[df_name] = df\n",
    "        \n",
    "        # Print summary statistics of the DataFrame\n",
    "        # print(df.describe())\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "# Define the scale factors\n",
    "scale_factors = {'x': 0.7575, 'y': 0.7575, 'z': 2.5}\n",
    "\n",
    "# Call the function for position_dirs_200901KK_st1 and position_dirs_201002KK_st3\n",
    "positions_ss = process_position_dirs(position_dirs_ss, scale_factors)\n",
    "positions_d5 = process_position_dirs(position_dirs_d5, scale_factors)\n",
    "positions_d10 = process_position_dirs(position_dirs_d10, scale_factors)\n",
    "positions_d15 = process_position_dirs(position_dirs_d15, scale_factors)\n",
    "positions_d30 = process_position_dirs(position_dirs_d30, scale_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Image Files (ims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the GFP images of the bones\n",
    "def get_ims_files_by_conditions():\n",
    "    # Lists to store .ims files for each condition\n",
    "    ims_files_d5 = []\n",
    "    ims_files_d10 = []\n",
    "    ims_files_d15 = []\n",
    "    ims_files_d30 = []\n",
    "    ims_files_ss = []\n",
    "    \n",
    "    # Find all directories in data_dir_5fu, excluding .ims files\n",
    "    all_dirs = [os.path.join(data_dir_5fu, d) for d in os.listdir(data_dir_5fu) if d.startswith('d')] \n",
    "    \n",
    "    # Classify the directories into d5, d10, d15, d30, or \"ss\"\n",
    "    for data_dir in all_dirs:\n",
    "        sub_dirs = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('ims') and 'GFP' in f]\n",
    "        if 'd5' in data_dir:\n",
    "            ims_files_d5.extend(sub_dirs)\n",
    "        elif 'd10' in data_dir:\n",
    "            ims_files_d10.extend(sub_dirs)\n",
    "        elif 'd15' in data_dir:\n",
    "            ims_files_d15.extend(sub_dirs)\n",
    "        elif 'd30' in data_dir:\n",
    "            ims_files_d30.extend(sub_dirs)\n",
    "        else:\n",
    "            # Files that don't contain 'd5', 'd10', 'd15', or 'd30' are classified as \"ss\"\n",
    "            ims_files_ss.extend(sub_dirs)\n",
    "\n",
    "    # Return a dictionary with the results\n",
    "    return {\n",
    "        'd5': ims_files_d5,\n",
    "        'd10': ims_files_d10,\n",
    "        'd15': ims_files_d15,\n",
    "        'd30': ims_files_d30,\n",
    "        'ss': ims_files_ss\n",
    "    }\n",
    "\n",
    "\n",
    "# Filter the ims files, keep the one with shorter name\n",
    "def filter_ims_files(ims_list):\n",
    "    # Dictionary to hold the shortest file for each unique prefix\n",
    "    grouped_files = defaultdict(list)\n",
    "\n",
    "    # Group filenames by the unique prefix after `d0/`\n",
    "    for filepath in ims_list:\n",
    "        # Extract the unique prefix (first two elements after 'd0/')\n",
    "        parts = filepath.split('/')\n",
    "        if len(parts) >= 3:\n",
    "            unique_prefix = parts[2].split()[0] + ' ' + parts[2].split()[1]  # Example: '200901KK st1'\n",
    "            grouped_files[unique_prefix].append(filepath)\n",
    "\n",
    "    # Filter to keep only the shortest name in each group\n",
    "    filtered_bone_ims = []\n",
    "    for prefix, files in grouped_files.items():\n",
    "        # Sort by filename length and take the shortest\n",
    "        shortest_file = min(files, key=len)\n",
    "        filtered_bone_ims.append(shortest_file)\n",
    "    return filtered_bone_ims\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Convert the bone images with non-zero values to dataframes with Position.X and Position.Y columns\n",
    "def image_to_df(image):\n",
    "    # Find the non-zero intensities and their coordinates\n",
    "    non_zero_coords = np.nonzero(image)  # Returns the y, x coordinates where intensity > 0\n",
    "    # intensity_values = image[non_zero_coords]  # Extract the intensity values\n",
    "\n",
    "    # Create a DataFrame from the non-zero coordinates and intensity values\n",
    "    df = pd.DataFrame({\n",
    "        'source': 'GFP',\n",
    "        'Position.Z': 0, #non_zero_coords[0],\n",
    "        'Position.Y': non_zero_coords[0],\n",
    "        'Position.X': non_zero_coords[1]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to process and store images for a given condition\n",
    "def process_bone_images(bone_ims_files):\n",
    "    bone_dict = {}\n",
    "    \n",
    "    for i in bone_ims_files:\n",
    "        # Extract bone name from the file path\n",
    "        bone_name = '_'.join(i.split('/')[-1].split(' ')[:2])\n",
    "        \n",
    "        # Load and process the image (assuming ims function is defined elsewhere)\n",
    "        bone_img = ims(i)  # You need to ensure that ims(i) correctly loads the image\n",
    "        # bone_img = bone_img[0, bone_img.shape[1] - 1, :, :, :].max(axis=0).copy()\n",
    "        bone_img = bone_img[0, 0, :, :, :].max(axis=0).copy()\n",
    "        # Morphological operations, labeling and remove small objects\n",
    "        # Threshold the image to create a binary mask\n",
    "        binary_img = bone_img > 0\n",
    "        # Label the binary image\n",
    "        labeled_img = measure.label(binary_img)\n",
    "        # Remove small objects\n",
    "        # TODO: we need to try out the min_size\n",
    "        cleaned_img = morphology.remove_small_objects(labeled_img, min_size=10000)\n",
    "        # Convert the cleaned image back to binary\n",
    "        bone_img_df = image_to_df(cleaned_img)\n",
    "        # Store the processed image in the dictionary with the extracted name\n",
    "        bone_dict[bone_name] = bone_img_df\n",
    "\n",
    "    return bone_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Access individual lists for each condition\n",
    "ims_files_by_conditions = get_ims_files_by_conditions()\n",
    "bone_ss_ims = ims_files_by_conditions['ss']\n",
    "bone_d5_ims = ims_files_by_conditions['d5']\n",
    "bone_d10_ims = ims_files_by_conditions['d10']\n",
    "bone_d15_ims = ims_files_by_conditions['d15']\n",
    "bone_d30_ims = ims_files_by_conditions['d30']\n",
    "\n",
    "# Filter the ims files, keep the one with shorter name\n",
    "filtered_bone_ss_ims = filter_ims_files(bone_ss_ims)\n",
    "filtered_bone_d5_ims = filter_ims_files(bone_d5_ims)\n",
    "filtered_bone_d10_ims = filter_ims_files(bone_d10_ims)\n",
    "filtered_bone_d15_ims = filter_ims_files(bone_d15_ims)\n",
    "filtered_bone_d30_ims = filter_ims_files(bone_d30_ims)\n",
    "\n",
    "# Process images for each condition\n",
    "bone_ss = process_bone_images(filtered_bone_ss_ims)\n",
    "bone_d5 = process_bone_images(filtered_bone_d5_ims)\n",
    "bone_d10 = process_bone_images(filtered_bone_d10_ims)\n",
    "bone_d15 = process_bone_images(filtered_bone_d15_ims)\n",
    "bone_d30 = process_bone_images(filtered_bone_d30_ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert Image to Dataframe and Combine both Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# From the positions_ss, positions_d5, positions_d10, positions_d15, positions_d30\n",
    "def positions_concat(positions_dict, bone_dict):\n",
    "    positions_df_dict = dict()\n",
    "    keys = {}\n",
    "    \n",
    "    # Create a mapping of dataset and sources\n",
    "    for key in positions_dict.keys():\n",
    "        dataset = '_'.join(key.split('_')[:2])\n",
    "        source = key.split('_')[-1]\n",
    "        if dataset not in keys:\n",
    "            keys[dataset] = [source]\n",
    "        else:\n",
    "            keys[dataset].append(source)\n",
    "    \n",
    "    # Concatenate DataFrames, adding a 'source' column\n",
    "    for dataset, sources in keys.items():\n",
    "        df = pd.concat(\n",
    "            [positions_dict[f'{dataset}_{source}'].assign(source=source) for source in sources], \n",
    "            ignore_index=True\n",
    "        )\n",
    "        df = pd.concat([df, bone_dict[dataset]], ignore_index=True)\n",
    "        positions_df_dict[dataset] = df\n",
    "    return positions_df_dict\n",
    "\n",
    "positions_ss_dfs = positions_concat(positions_ss, bone_ss)\n",
    "positions_d5_dfs = positions_concat(positions_d5, bone_d5)\n",
    "positions_d10_dfs = positions_concat(positions_d10, bone_d10)\n",
    "positions_d15_dfs = positions_concat(positions_d15, bone_d15)\n",
    "positions_d30_dfs = positions_concat(positions_d30, bone_d30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# delete bone_ss, bone_d5, bone_d10, bone_d15, bone_d30 to free up memory\n",
    "del bone_ss, bone_d5, bone_d10, bone_d15, bone_d30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.4 Load Reference Bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_bone_dir = 'data_metabone_5fu/reference_bone/200117KK st4 acat27a 5FU d5 mBone(1)  mGFP(2).ims'\n",
    "reference_bone = ims(reference_bone_dir)\n",
    "\n",
    "# ref_bone_outer = reference_bone[0,0,:,:,:].max(axis=0).copy()\n",
    "ref_bone_inner = reference_bone[0,1,:,:,:].max(axis=0).copy()\n",
    "\n",
    "# ref_bone_outer = image_to_df(ref_bone_outer)\n",
    "ref_bone_inner = image_to_df(ref_bone_inner)\n",
    "\n",
    "# Flip the y-axis to match the image coordinates\n",
    "y_max = np.max((ref_bone_outer['Position.Y'].max(), ref_bone_inner['Position.Y'].max()))\n",
    "# ref_bone_outer['Position.Y'] = y_max - ref_bone_outer['Position.Y']\n",
    "ref_bone_inner['Position.Y'] = y_max - ref_bone_inner['Position.Y']\n",
    "\n",
    "# delete reference bone to save memory\n",
    "del reference_bone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Inspection and Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the x, y for each df \n",
    "for test_df in [positions_ss_dfs, positions_d5_dfs, positions_d10_dfs, positions_d15_dfs, positions_d30_dfs]:\n",
    "    fig, axs = plt.subplots(len(test_df.keys()), 7, figsize=(12*len(test_df.keys()), 36))\n",
    "\n",
    "    # Visualize the steady state positions row by key and the columns are source\n",
    "    for i, key in enumerate(test_df.keys()):\n",
    "        for j, source in enumerate(test_df[key]['source'].unique()):\n",
    "            if source == 'GFP':\n",
    "                sample_number = 10000\n",
    "                source_df = test_df[key].loc[test_df[key]['source'] == source, ['Position.X', 'Position.Y']].sample(sample_number)\n",
    "            else:\n",
    "                source_df = test_df[key].loc[test_df[key]['source'] == source, ['Position.X', 'Position.Y']]\n",
    "            ax = axs[i, j]\n",
    "            ax.scatter(source_df['Position.X'], source_df['Position.Y'], s=1)\n",
    "            ax.set_title(f\"{key} - {source}\")\n",
    "            ax.axis('equal')\n",
    "            ax.set_xlabel('Position.X')\n",
    "            ax.set_ylabel('Position.Y')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    # fig.savefig(f'{save_dir}/{datetime.today().strftime(\"%y%m%d\")}_YL_raw_d5_d10_d15_d30.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 2.2 Data Adjustments (Flipping Coordinates, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the visual check above, we need to flip the y axis for dataset with key \n",
    "# ss: '201002KK_st5'\n",
    "# d5: '200117KK_st3', \n",
    "# d10: '200110KK_st3', \n",
    "# d15: '200908KK_st4', '240819JH_st2'\n",
    "# The flip should be applied to all sources but the shift should only based on the MK source\n",
    "def y_flip(positions_df_dict, keys_to_flip):\n",
    "    for key in keys_to_flip:\n",
    "        # y_min = positions_df_dict[key]['Position.Y'].min()\n",
    "        y_max = positions_df_dict[key]['Position.Y'].max()\n",
    "        # Flip the y axis, we don't care about the source\n",
    "        positions_df_dict[key]['Position.Y'] = y_max - positions_df_dict[key]['Position.Y']\n",
    "    return positions_df_dict\n",
    "\n",
    "positions_ss_dfs = y_flip(positions_ss_dfs, ['201002KK_st5', '200901KK_st3', '210518KK_st12', '210518KK_st1', '200728KK_st2'])\n",
    "positions_d5_dfs = y_flip(positions_d5_dfs, ['200117KK_st3', '200117KK_st5', '200117KK_st4','200117KK_st1'])\n",
    "positions_d10_dfs = y_flip(positions_d10_dfs, ['200110KK_st3','200110KK_st1', '191202KK_st1', '200110KK_st4', '200110KK_st8'])\n",
    "positions_d15_dfs = y_flip(positions_d15_dfs, ['240819JH_st2', '201002KK_st7', '201020KK_st1', '200901KK_st5', '200728KK_st3', '200908KK_st4', '210108KK_st10'])\n",
    "positions_d30_dfs = y_flip(positions_d30_dfs, ['200901KK_st7', '201103KKI_st4', '201103KK_st2', '200908KK_st2', '240926JH_st2'])\n",
    "\n",
    "\n",
    "def x_flip(positions_df_dict, keys_to_flip):\n",
    "    for key in keys_to_flip:\n",
    "        # x_min = positions_df_dict[key]['Position.X'].min()\n",
    "        x_max = positions_df_dict[key]['Position.X'].max()\n",
    "        # Flip the x axis, we don't care about the source\n",
    "        positions_df_dict[key]['Position.X'] = x_max - positions_df_dict[key]['Position.X']\n",
    "    return positions_df_dict\n",
    "\n",
    "positions_d5_dfs = x_flip(positions_d5_dfs, ['200117KK_st5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the x, y for each df again after flipping the coordinates\n",
    "for test_df in [positions_ss_dfs, positions_d5_dfs, positions_d10_dfs, positions_d15_dfs, positions_d30_dfs]:\n",
    "    fig, axs = plt.subplots(len(test_df.keys()), 7, figsize=(12*len(test_df.keys()), 36))\n",
    "\n",
    "    # Visualize the steady state positions row by key and the columns are source\n",
    "    for i, key in enumerate(test_df.keys()):\n",
    "        for j, source in enumerate(test_df[key]['source'].unique()):\n",
    "            if source == 'GFP':\n",
    "                sample_number = 10000\n",
    "                source_df = test_df[key].loc[test_df[key]['source'] == source, ['Position.X', 'Position.Y']].sample(sample_number)\n",
    "            else:\n",
    "                source_df = test_df[key].loc[test_df[key]['source'] == source, ['Position.X', 'Position.Y']]\n",
    "            ax = axs[i, j]\n",
    "            ax.scatter(source_df['Position.X'], source_df['Position.Y'], s=1)\n",
    "            ax.set_title(f\"{key} - {source}\")\n",
    "            ax.axis('equal')\n",
    "            ax.set_xlabel('Position.X')\n",
    "            ax.set_ylabel('Position.Y')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    # fig.savefig(f'{save_dir}/{datetime.today().strftime(\"%y%m%d\")}_YL_raw_d5_d10_d15_d30.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bone Alignment and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Align Bones to the Reference Bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 2D outline of the bone in the x-y plane\n",
    "def find_outline(points, window_size=10):\n",
    "    \"\"\"\n",
    "    Find the outline of a set of points by finding the min and max y-values for each x-value within a window.\n",
    "    The outline is only in the x-y plane.\n",
    "    Parameters:\n",
    "    points : np.array of shape (n,2) points.\n",
    "    window_size : size of the window to smooth the outline.\n",
    "    Returns:\n",
    "    outline_points : np.array of outline points.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(points, columns=['x', 'y'])\n",
    "    \n",
    "    min_y_points = []\n",
    "    max_y_points = []\n",
    "\n",
    "    # Sort points by x value\n",
    "    df_sorted = df.sort_values(by='x')\n",
    "    \n",
    "    # Slide over the x values with a window\n",
    "    for i in range(0, len(df_sorted), window_size):\n",
    "        window = df_sorted.iloc[i:i + window_size]\n",
    "        min_y = window.loc[window['y'].idxmin()]\n",
    "        max_y = window.loc[window['y'].idxmax()]\n",
    "        min_y_points.append(min_y)\n",
    "        max_y_points.append(max_y)\n",
    "    \n",
    "    # Ensure the outline is in order\n",
    "    min_y_points = pd.DataFrame(min_y_points).drop_duplicates().sort_values(by='x').values\n",
    "    max_y_points = pd.DataFrame(max_y_points).drop_duplicates().sort_values(by='x', ascending=False).values\n",
    "\n",
    "    # Combine min_y and max_y points and close the loop\n",
    "    outline_points = np.vstack([min_y_points, max_y_points, min_y_points[0]])\n",
    "\n",
    "    return outline_points\n",
    "\n",
    "# 0. Calculate the overlap\n",
    "# 1. Find the outline of each bone based on the x-y plane\n",
    "\n",
    "# 2. Find the center of each bone (outline) and put the center of the bones at the same position\n",
    "def calculate_centroid(outline_points):\n",
    "    \"\"\"\n",
    "    Calculate the centroid of the bone outline.\n",
    "    \n",
    "    Parameters:\n",
    "    outline_points : np.array of shape (n, 2)\n",
    "    \n",
    "    Returns:\n",
    "    centroid : tuple containing (centroid_x, centroid_y)\n",
    "    \"\"\"\n",
    "    # Use the weight centroid\n",
    "    centroid_x = np.mean(outline_points[:, 0])\n",
    "    centroid_y = np.mean(outline_points[:, 1])\n",
    "    \n",
    "    # Using the middle of the x and y values as the centroid\n",
    "    # x_min, x_max = outline_points[:, 0].min(), outline_points[:, 0].max()\n",
    "    # y_min, y_max = outline_points[:, 1].min(), outline_points[:, 1].max()\n",
    "    # centroid_x = (x_min + x_max) / 2\n",
    "    # centroid_y = (y_min + y_max) / 2\n",
    "    return centroid_x, centroid_y\n",
    "def translate_to_origin(outline_points, centroid):\n",
    "    \"\"\"\n",
    "    Translate the outline points so that the centroid is at the origin.\n",
    "    \n",
    "    Parameters:\n",
    "    outline_points : np.array of shape (n, 2)\n",
    "    centroid : tuple containing (centroid_x, centroid_y)\n",
    "    \n",
    "    Returns:\n",
    "    translated_points : np.array of shape (n, 2)\n",
    "    \"\"\"\n",
    "    translated_points = outline_points.astype(np.float64).copy()\n",
    "    \n",
    "    translated_points[:, 0] -= centroid[0]\n",
    "    translated_points[:, 1] -= centroid[1]\n",
    "    return translated_points\n",
    "# 3. Rescale the bones to the same size (bounding box)(optional)\n",
    "def get_max_dimensions(bone_dicts):\n",
    "    \"\"\"\n",
    "    Find the maximum width and height across all bone outlines in the given dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    bone_dicts : list of dictionaries of bones (where each value is a DataFrame with 'Position.X' and 'Position.Y')\n",
    "    \n",
    "    Returns:\n",
    "    max_width : float, maximum width found across all bones\n",
    "    max_height : float, maximum height found across all bones\n",
    "    \"\"\"\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "    \n",
    "    for bone_dict in bone_dicts:\n",
    "        for df in bone_dict.values():\n",
    "            # Extract bone points where 'source' == 'Bone'\n",
    "            bone_points = df[df['source'] == 'Bone'][['Position.X', 'Position.Y']].values\n",
    "            \n",
    "            # Find min and max values of x and y\n",
    "            min_x, max_x = bone_points[:, 0].min(), bone_points[:, 0].max()\n",
    "            min_y, max_y = bone_points[:, 1].min(), bone_points[:, 1].max()\n",
    "            \n",
    "            # Calculate width and height\n",
    "            width = max_x - min_x\n",
    "            height = max_y - min_y\n",
    "            \n",
    "            # Update maximum width and height if necessary\n",
    "            if width > max_width:\n",
    "                max_width = width\n",
    "            if height > max_height:\n",
    "                max_height = height\n",
    "                \n",
    "    return max_width, max_height\n",
    "def rescale_outline(outline_points, max_width, max_height):\n",
    "    \"\"\"\n",
    "    Rescale the bone outline to fit within the maximum width and height across all bones.\n",
    "    \n",
    "    Parameters:\n",
    "    outline_points : np.array of shape (n, 2)\n",
    "    max_width : float, the maximum width across all bones\n",
    "    max_height : float, the maximum height across all bones\n",
    "    \n",
    "    Returns:\n",
    "    scaled_points : np.array of shape (n, 2)\n",
    "    \"\"\"\n",
    "    min_x, max_x = outline_points[:, 0].min(), outline_points[:, 0].max()\n",
    "    min_y, max_y = outline_points[:, 1].min(), outline_points[:, 1].max()\n",
    "\n",
    "    current_width = max_x - min_x\n",
    "    current_height = max_y - min_y\n",
    "\n",
    "    scale_x = max_width / current_width\n",
    "    scale_y = max_height / current_height\n",
    "\n",
    "    scaled_points = outline_points.copy()\n",
    "    scaled_points[:, 0] *= scale_x\n",
    "    scaled_points[:, 1] *= scale_y\n",
    "\n",
    "    return scaled_points\n",
    "\n",
    "# Calculate the overlap area on the grid inside the outline\n",
    "def calculate_overlap_area(reference_outline, target_outline, resolution=200):\n",
    "    \"\"\"\n",
    "    Calculate the overlap area (in terms of pixels or points) between two outlines.\n",
    "    \n",
    "    Parameters:\n",
    "    reference_outline : np.array of shape (n, 2), outline of the reference bone\n",
    "    target_outline : np.array of shape (n, 2), outline of the target bone\n",
    "    resolution : int, the number of points or pixels to use for the area calculation.\n",
    "    \n",
    "    Returns:\n",
    "    overlap_area : float, the number of pixels or points where the areas overlap.\n",
    "    \"\"\"\n",
    "    # Ensure the outlines are 2D arrays of shape (N, 2)\n",
    "    reference_outline = np.asarray(reference_outline).reshape(-1, 2)\n",
    "    target_outline = np.asarray(target_outline).reshape(-1, 2)\n",
    "    \n",
    "    # Calculate centroids for both bones\n",
    "    reference_centroid = calculate_centroid(reference_outline)\n",
    "    target_centroid = calculate_centroid(target_outline)\n",
    "    \n",
    "    # Translate both bones to center them\n",
    "    reference_outline_centered = translate_to_origin(reference_outline, reference_centroid)\n",
    "    target_outline_centered = translate_to_origin(target_outline, target_centroid)\n",
    "    \n",
    "    # Get bounding box of the reference outline\n",
    "    min_x, max_x = reference_outline_centered[:, 0].min(), reference_outline_centered[:, 0].max()\n",
    "    min_y, max_y = reference_outline_centered[:, 1].min(), reference_outline_centered[:, 1].max()\n",
    "    \n",
    "    # Generate grid of points (pixels) covering the bounding box\n",
    "    x_grid = np.linspace(min_x, max_x, resolution)\n",
    "    y_grid = np.linspace(min_y, max_y, resolution)\n",
    "    xv, yv = np.meshgrid(x_grid, y_grid)\n",
    "    grid_points = np.vstack([xv.ravel(), yv.ravel()]).T\n",
    "\n",
    "    # Create Path objects for the reference and target outlines\n",
    "    reference_path = Path(reference_outline_centered)\n",
    "    target_path = Path(target_outline_centered)\n",
    "    \n",
    "    # Check which points of the grid are inside both outlines\n",
    "    points_in_reference = reference_path.contains_points(grid_points)\n",
    "    points_in_target = target_path.contains_points(grid_points)\n",
    "    \n",
    "    # Calculate the overlap area as the number of points (pixels) inside both outlines\n",
    "    overlap_area = np.sum(points_in_reference & points_in_target)\n",
    "    \n",
    "    return overlap_area\n",
    "\n",
    "def rotate_points(points, angle):\n",
    "    \"\"\"\n",
    "    Rotate a set of points by a given angle.\n",
    "    \n",
    "    Parameters:\n",
    "    points : np.array of shape (n, 2)\n",
    "    angle : float, angle to rotate by in radians\n",
    "    \n",
    "    Returns:\n",
    "    rotated_points : np.array of shape (n, 2)\n",
    "    \"\"\"\n",
    "    # Ensure points are a 2D array of shape (N, 2)\n",
    "    points = np.asarray(points).reshape(-1, 2)\n",
    "    \n",
    "    # Rotation matrix\n",
    "    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], \n",
    "                                [np.sin(angle), np.cos(angle)]])\n",
    "    \n",
    "    # Rotate points\n",
    "    rotated_points = points.dot(rotation_matrix)\n",
    "    \n",
    "    return rotated_points\n",
    "\n",
    "# TODO: for some reason the function seems only test on the initial guesses, so now we are using the grid search\n",
    "def optimize_rotation(reference_outline, target_outline):\n",
    "    \"\"\"\n",
    "    Optimize the rotation of the target bone to maximize overlap with the reference bone,\n",
    "    constrained to -90 to 90 degrees.\n",
    "    \n",
    "    Parameters:\n",
    "    reference_outline : np.array of shape (n, 2), outline of the reference bone\n",
    "    target_outline : np.array of shape (n, 2), outline of the target bone\n",
    "    \n",
    "    Returns:\n",
    "    optimal_rotated_points : np.array of shape (n, 2), the rotated and recentered target bone outline with maximum overlap\n",
    "    optimal_angle : float, the optimal rotation angle in radians\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective_function(angle):\n",
    "        # Rotate the target outline by the current angle\n",
    "        rotated_outline = rotate_points(target_outline, angle)\n",
    "        # Calculate the negative overlap (since we are minimizing)\n",
    "        return -calculate_overlap_area(reference_outline, rotated_outline)\n",
    "\n",
    "    # Try different initial guesses for the angle to improve convergence\n",
    "    initial_guesses = [-0.1, 0, 0.1]  # You can try different values here\n",
    "    \n",
    "    best_result = None\n",
    "    best_angle = None\n",
    "    best_rotated_points = None\n",
    "\n",
    "    for x0 in initial_guesses:\n",
    "        # Perform optimization with the initial guess x0\n",
    "        result = minimize(objective_function, x0=x0, bounds=[(-np.pi/4, np.pi/4)], method='L-BFGS-B', options={'maxiter': 50})\n",
    "        \n",
    "        # Check if this result is valid and better than the current best\n",
    "        if result.success:\n",
    "            if best_result is None or result.fun < best_result:\n",
    "                best_result = result.fun\n",
    "                best_angle = result.x[0]\n",
    "                best_rotated_points = rotate_points(target_outline, best_angle)\n",
    "\n",
    "    if best_rotated_points is None:\n",
    "        raise ValueError(\"Optimization failed to converge.\")\n",
    "\n",
    "    # Recenter the final rotated outline\n",
    "    final_centroid = calculate_centroid(best_rotated_points)\n",
    "    best_rotated_points = translate_to_origin(best_rotated_points, final_centroid)\n",
    "    \n",
    "    return best_rotated_points, best_angle, final_centroid\n",
    "\n",
    "def grid_search_rotation(reference_outline, target_outline, angle_step=np.pi/36):\n",
    "    \"\"\"\n",
    "    Perform a grid search over possible rotation angles to maximize overlap.\n",
    "    \n",
    "    Parameters:\n",
    "    reference_outline : np.array of shape (n, 2), outline of the reference bone\n",
    "    target_outline : np.array of shape (n, 2), outline of the target bone\n",
    "    angle_step : float, step size for angle search (in radians)\n",
    "    \n",
    "    Returns:\n",
    "    best_rotated_points : np.array of shape (n, 2), the rotated target bone outline with maximum overlap\n",
    "    best_angle : float, the optimal rotation angle in radians\n",
    "    \"\"\"\n",
    "    best_angle = None\n",
    "    max_overlap = -np.inf\n",
    "    best_rotated_points = None\n",
    "    \n",
    "    # Iterate over angles between -90 and 90 degrees (in radians)\n",
    "    for angle in np.arange(-np.pi/4, np.pi/4, angle_step):\n",
    "        rotated_outline = rotate_points(target_outline, angle)\n",
    "        overlap = calculate_overlap_area(reference_outline, rotated_outline)\n",
    "        \n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_angle = angle\n",
    "            best_rotated_points = rotated_outline\n",
    "    \n",
    "    # Recenter the final rotated outline\n",
    "    final_centroid = calculate_centroid(best_rotated_points)\n",
    "    best_rotated_points = translate_to_origin(best_rotated_points, final_centroid)\n",
    "    \n",
    "    return best_rotated_points, best_angle, final_centroid\n",
    "\n",
    "# Modify the process_and_align_bones function to accept the max_width and max_height\n",
    "def process_and_align_bones_with_overlap(bone_dict, reference_bone_name='201002KK_st5', window_size=500):\n",
    "    \"\"\"\n",
    "    Process and align all bones from the given dictionary to maximize overlap with a reference bone.\n",
    "    \n",
    "    Parameters:\n",
    "    bone_dict : dict of bones, where each value is a DataFrame with 'Position.X' and 'Position.Y'.\n",
    "    reference_bone_name : string, the name of the bone to use as the reference for alignment.\n",
    "    \n",
    "    Returns:\n",
    "    aligned_bones : dict of aligned bone outlines\n",
    "    \"\"\"\n",
    "    aligned_bones = {}\n",
    "    aligned_angles = {}\n",
    "    aligned_centroids = {}\n",
    "    # Check the type of the reference_bone_name\n",
    "    # If it is a dataframe, we can use the reference_bone_name directly\n",
    "    if isinstance(reference_bone_name, pd.DataFrame):\n",
    "        reference_df = reference_bone_name\n",
    "        reference_points = reference_df[reference_df['source'] == 'GFP'][['Position.X', 'Position.Y']].values\n",
    "        reference_outline = find_outline(reference_points, window_size=window_size)\n",
    "        reference_centroid = calculate_centroid(reference_outline)\n",
    "        reference_outline = translate_to_origin(reference_outline, reference_centroid)\n",
    "        \n",
    "        for bone_name, df in bone_dict.items():\n",
    "\n",
    "            # Filter points where 'source' == 'Bone'\n",
    "            bone_points = df[df['source'] == 'GFP'][['Position.X', 'Position.Y']].values\n",
    "        \n",
    "            # Find the outline of the target bone\n",
    "            target_outline = find_outline(bone_points, window_size=window_size)\n",
    "            # Optimize rotation to maximize overlap with the reference bone\n",
    "            aligned_outline, best_angle, final_centroid = grid_search_rotation(reference_outline, target_outline)\n",
    "            aligned_bones[bone_name] = aligned_outline\n",
    "            aligned_angles[bone_name] = best_angle\n",
    "            aligned_centroids[bone_name] = final_centroid\n",
    "\n",
    "        \n",
    "    elif isinstance(reference_bone_name, str):\n",
    "            \n",
    "        \n",
    "        # Extract the reference bone outline\n",
    "        reference_df = positions_ss_dfs[reference_bone_name]\n",
    "        reference_points = reference_df[reference_df['source'] == 'Bone'][['Position.X', 'Position.Y']].values\n",
    "\n",
    "        reference_outline = find_outline(reference_points, window_size=window_size)\n",
    "        # Recenter the reference outline\n",
    "        reference_centroid = calculate_centroid(reference_outline)\n",
    "        reference_outline = translate_to_origin(reference_outline, reference_centroid)\n",
    "        \n",
    "        for bone_name, df in bone_dict.items():\n",
    "\n",
    "            if bone_name == reference_bone_name:\n",
    "                # Keep the reference bone as is\n",
    "                aligned_bones[bone_name] = reference_outline\n",
    "                aligned_angles[bone_name] = 0\n",
    "                aligned_centroids[bone_name] = reference_centroid\n",
    "            else:\n",
    "                # Filter points where 'source' == 'Bone'\n",
    "                bone_points = df[df['source'] == 'Bone'][['Position.X', 'Position.Y']].values\n",
    "            \n",
    "                # Find the outline of the target bone\n",
    "                target_outline = find_outline(bone_points, window_size=window_size)\n",
    "                # Optimize rotation to maximize overlap with the reference bone\n",
    "                aligned_outline, best_angle, final_centroid = grid_search_rotation(reference_outline, target_outline)\n",
    "                aligned_bones[bone_name] = aligned_outline\n",
    "                aligned_angles[bone_name] = best_angle\n",
    "                aligned_centroids[bone_name] = final_centroid\n",
    "    \n",
    "    return aligned_bones, aligned_angles, aligned_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the outline found based on the ref_bone_outer and the ref_bone_inner\n",
    "# recenter and find the outline\n",
    "# ref_bone_outer_points = ref_bone_outer[ref_bone_outer['source'] == 'GFP'][['Position.X', 'Position.Y']].values\n",
    "ref_bone_inner_points = ref_bone_inner[ref_bone_inner['source'] == 'GFP'][['Position.X', 'Position.Y']].values\n",
    "\n",
    "# ef_bone_outer_outline = find_outline(ref_bone_outer_points, window_size=500)\n",
    "ref_bone_inner_outline = find_outline(ref_bone_inner_points, window_size=500)\n",
    "# ref_bone_outer_centroid = calculate_centroid(ref_bone_outer_outline)\n",
    "ref_bone_inner_centroid = calculate_centroid(ref_bone_inner_outline)\n",
    "\n",
    "# The two shared the same centroid\n",
    "# ref_bone_outer_outline = translate_to_origin(ref_bone_outer_outline, ref_bone_outer_centroid)\n",
    "# ref_bone_inner_outline = translate_to_origin(ref_bone_inner_outline, ref_bone_outer_centroid)\n",
    "ref_bone_inner_outline = translate_to_origin(ref_bone_inner_outline, ref_bone_inner_centroid)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# ax.plot(ref_bone_outer_outline[:, 0], ref_bone_outer_outline[:, 1], color='r', label='Outer')\n",
    "ax.plot(ref_bone_inner_outline[:, 0], ref_bone_inner_outline[:, 1], color='b', label='Inner')\n",
    "ax.set_title('Outer and Inner Bone Outlines')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the process to all bone dictionaries using the max width and height\n",
    "aligned_ss_bone_outlines, aligned_ss_angles, aligned_ss_centroids = process_and_align_bones_with_overlap(positions_ss_dfs, reference_bone_name=ref_bone_inner, window_size=500)\n",
    "aligned_d5_bone_outlines, aligned_d5_angles, aligned_d5_centroids= process_and_align_bones_with_overlap(positions_d5_dfs, reference_bone_name=ref_bone_inner, window_size=500)\n",
    "aligned_d10_bone_outlines, aligned_d10_angles, aligned_d10_centroids = process_and_align_bones_with_overlap(positions_d10_dfs, reference_bone_name=ref_bone_inner, window_size=500)\n",
    "aligned_d15_bone_outlines, aligned_d15_angles, aligned_d15_centroids = process_and_align_bones_with_overlap(positions_d15_dfs, reference_bone_name=ref_bone_inner, window_size=500)\n",
    "aligned_d30_bone_outlines, aligned_d30_angles, aligned_d30_centroids = process_and_align_bones_with_overlap(positions_d30_dfs, reference_bone_name=ref_bone_inner, window_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied the rotation and recenter to all the populations and save the aligned bones\n",
    "\n",
    "def align_bones_with_centroids_angles(positions_df, aligned_centroids, aligned_angles):\n",
    "    aligned_bones = {}\n",
    "    \n",
    "    for bone_name, df in positions_df.items():\n",
    "        # Get the centroid of the bone\n",
    "        centroid = aligned_centroids[bone_name]\n",
    "        # Get the angle to rotate\n",
    "        angle = aligned_angles[bone_name]\n",
    "\n",
    "        # The order of the rotation and the translation matters!\n",
    "        # The centroids are based on the rotated and translated points\n",
    "        # Rotate the df\n",
    "        df_rotated = df.copy()\n",
    "        # Exclude the Bone points\n",
    "        # TODO: if we still want to find the combined outline, then we will need to perform the rotation and recentering on the 'Bone' as well\n",
    "        # df_rotated = df_rotated[df_rotated['source'] != 'Bone']\n",
    "        df_rotated[['Position.X', 'Position.Y']] = rotate_points(df_rotated[['Position.X', 'Position.Y']].values, angle)\n",
    "        \n",
    "        # Translate the df\n",
    "        df_translated = df_rotated.copy()\n",
    "        df_translated['Position.X'] -= centroid[0]\n",
    "        df_translated['Position.Y'] -= centroid[1]\n",
    "        \n",
    "        aligned_bones[bone_name] = df_translated\n",
    "    return aligned_bones\n",
    "\n",
    "\n",
    "aligned_ss_bones = align_bones_with_centroids_angles(positions_ss_dfs, aligned_ss_centroids, aligned_ss_angles)\n",
    "aligned_d5_bones = align_bones_with_centroids_angles(positions_d5_dfs, aligned_d5_centroids, aligned_d5_angles)\n",
    "aligned_d10_bones = align_bones_with_centroids_angles(positions_d10_dfs, aligned_d10_centroids, aligned_d10_angles)\n",
    "aligned_d15_bones = align_bones_with_centroids_angles(positions_d15_dfs, aligned_d15_centroids, aligned_d15_angles)\n",
    "aligned_d30_bones = align_bones_with_centroids_angles(positions_d30_dfs, aligned_d30_centroids, aligned_d30_angles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "only will be used if we want to compare the aligned data with transformed data\n",
    "# Concat all the dataframe from aligned_bones\n",
    "aligned_positions_ss_df = pd.concat(aligned_ss_bones)\n",
    "aligned_positions_d5_df = pd.concat(aligned_d5_bones)\n",
    "aligned_positions_d10_df = pd.concat(aligned_d10_bones)\n",
    "aligned_positions_d15_df = pd.concat(aligned_d15_bones)\n",
    "aligned_positions_d30_df = pd.concat(aligned_d30_bones)\n",
    "\n",
    "# Before we save the file, we exclude the data when source is 'Bone'\n",
    "aligned_positions_ss_df = aligned_positions_ss_df[aligned_positions_ss_df['source'] != 'GFP']\n",
    "aligned_positions_d5_df = aligned_positions_d5_df[aligned_positions_d5_df['source'] != 'GFP']\n",
    "aligned_positions_d10_df = aligned_positions_d10_df[aligned_positions_d10_df['source'] != 'GFP']\n",
    "aligned_positions_d15_df = aligned_positions_d15_df[aligned_positions_d15_df['source'] != 'GFP']\n",
    "aligned_positions_d30_df = aligned_positions_d30_df[aligned_positions_d30_df['source'] != 'GFP']\n",
    "\n",
    "# Save the data points \n",
    "aligned_positions_ss_df.to_csv(f'{save_dir}/aligned_positions_ss_df.csv', index=False)\n",
    "aligned_positions_d5_df.to_csv(f'{save_dir}/aligned_positions_d5_df.csv', index=False)\n",
    "aligned_positions_d10_df.to_csv(f'{save_dir}/aligned_positions_d10_df.csv', index=False)\n",
    "aligned_positions_d15_df.to_csv(f'{save_dir}/aligned_positions_d15_df.csv', index=False)\n",
    "aligned_positions_d30_df.to_csv(f'{save_dir}/aligned_positions_d30_df.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete positions_ss_dfs, positions_d5_dfs, positions_d10_dfs, positions_d15_dfs, positions_d30_dfs to save memory\n",
    "# These dataframes are important for the alignment, outline creation\n",
    "# Of course, we can create the new outline based on the aligned data (dict)\n",
    "# TODO: delete them after we confirm the outline for each bone\n",
    "# del positions_ss_dfs, positions_d5_dfs, positions_d10_dfs, positions_d15_dfs, positions_d30_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transform Bone Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the outlines\n",
    "def smooth_outline(outline, sigma=2):\n",
    "    if not np.array_equal(outline[0], outline[-1]):\n",
    "        outline = np.vstack([outline, outline[0]])\n",
    "    smoothed_x = gaussian_filter1d(outline[:, 0], sigma=sigma)\n",
    "    smoothed_y = gaussian_filter1d(outline[:, 1], sigma=sigma)\n",
    "    smoothed_outline = np.vstack((smoothed_x, smoothed_y)).T\n",
    "    if not np.array_equal(smoothed_outline[0], smoothed_outline[-1]):\n",
    "        smoothed_outline = np.vstack([smoothed_outline, smoothed_outline[0]])\n",
    "    return smoothed_outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the reference bone outlines\n",
    "ref_bone_inner_smoothed = smooth_outline(ref_bone_inner_outline, sigma=80)\n",
    "common_outline = ref_bone_inner_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed the alinged_bone_outlines\n",
    "aligned_ss_bone_outlines_smoothed = {}\n",
    "aligned_d5_bone_outlines_smoothed = {}\n",
    "aligned_d10_bone_outlines_smoothed = {}\n",
    "aligned_d15_bone_outlines_smoothed = {}\n",
    "aligned_d30_bone_outlines_smoothed = {}\n",
    "# TODO: problem with the d15 bone outline (because of the discontinious tail)\n",
    "sigma = 50\n",
    "for bone_name, outline in aligned_ss_bone_outlines.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    aligned_ss_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in aligned_d5_bone_outlines.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    aligned_d5_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in aligned_d10_bone_outlines.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    aligned_d10_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in aligned_d15_bone_outlines.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    aligned_d15_bone_outlines_smoothed[bone_name] = smoothed_outline\n",
    "for bone_name, outline in aligned_d30_bone_outlines.items():\n",
    "    smoothed_outline = smooth_outline(outline, sigma=sigma)\n",
    "    aligned_d30_bone_outlines_smoothed[bone_name] = smoothed_outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_range_at_x(shape_points, x):\n",
    "    \"\"\"\n",
    "    Find the range of y-values where the vertical line at x intersects the shape.\n",
    "    \"\"\"\n",
    "    # Find all edges of the shape where x is between the x-coordinates of the endpoints\n",
    "    y_vals = []\n",
    "    for i in range(len(shape_points)):\n",
    "        p1 = shape_points[i]\n",
    "        p2 = shape_points[(i + 1) % len(shape_points)]  # wrap around the shape points\n",
    "        \n",
    "        # Check if the x value is between p1 and p2's x-coordinates\n",
    "        if (p1[0] <= x <= p2[0]) or (p2[0] <= x <= p1[0]):\n",
    "            # Linearly interpolate to find the corresponding y value at x\n",
    "            if p1[0] != p2[0]:  # Avoid division by zero\n",
    "                y = p1[1] + (p2[1] - p1[1]) * (x - p1[0]) / (p2[0] - p1[0])\n",
    "                y_vals.append(y)\n",
    "    \n",
    "    if y_vals:\n",
    "        return min(y_vals), max(y_vals)\n",
    "    else:\n",
    "        return None, None  # No intersection with the shape at this x\n",
    "\n",
    "def create_structured_grid(shape_points, x_num, y_num):\n",
    "    \"\"\"\n",
    "    Create a structured grid by dividing the bounding box of the shape into x_num vertical sections.\n",
    "    Then place y_num points along each vertical grid line where it intersects the shape.\n",
    "    \"\"\"\n",
    "    shape_points = np.array(shape_points)\n",
    "    \n",
    "    # Step 1: Compute the bounding box\n",
    "    min_x, max_x = np.min(shape_points[:, 0]), np.max(shape_points[:, 0])\n",
    "    \n",
    "    # Step 2: Divide the x-range into equal sections\n",
    "    x_vals = np.linspace(min_x, max_x, x_num + 1)  # x_num divisions create x_num + 1 grid lines\n",
    "    # Shift the x_vals to the left by half the grid spacing to center the grid\n",
    "    x_vals = x_vals[1:]  # Remove the first point (left edge of bounding box)\n",
    "    x_vals = x_vals - (x_vals[1] - x_vals[0]) / 2  # Shift left by half the grid spacing\n",
    "    \n",
    "    grid_points = []\n",
    "\n",
    "    # Step 3: For each x grid line, find the y range and then place points\n",
    "    for x in x_vals:  # Skip the first and last lines (already have the bounding box)\n",
    "        y_min, y_max = get_y_range_at_x(shape_points, x)\n",
    "        \n",
    "        if y_min is not None and y_max is not None:\n",
    "            # Get y points by placing y_num points between y_min and y_max\n",
    "            y_vals = np.linspace(y_min, y_max, y_num+1)\n",
    "            \n",
    "            # Shift the y_vals down by half the grid spacing to center the grid\n",
    "            y_vals = y_vals[1:]  # Remove the first point (bottom edge of bounding box)\n",
    "            y_vals = y_vals - (y_vals[1] - y_vals[0]) / 2  # Shift down by half the grid spacing\n",
    "            # Add grid points (x, y) for this vertical line\n",
    "            for y in y_vals: # Skip the first and last points (already have the y_min and y_max)\n",
    "                grid_points.append([x, y])\n",
    "\n",
    "    return np.array(grid_points)\n",
    "\n",
    "def is_point_inside_shape(point, shape_points):\n",
    "    \"\"\"\n",
    "    Determines if a point is inside an irregular shape using ray-casting.\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    n = len(shape_points)\n",
    "    inside = False\n",
    "    p1x, p1y = shape_points[0]\n",
    "    for i in range(n + 1):\n",
    "        p2x, p2y = shape_points[i % n]\n",
    "        if y > min(p1y, p2y):\n",
    "            if y <= max(p1y, p2y):\n",
    "                if x <= max(p1x, p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                    if p1x == p2x or x <= xinters:\n",
    "                        inside = not inside\n",
    "        p1x, p1y = p2x, p2y\n",
    "    return inside\n",
    "\n",
    "\n",
    "def thin_plate_spline_transform(src_points, dst_points):\n",
    "    \"\"\"\n",
    "    Perform Thin Plate Spline (TPS) transformation from src_points to dst_points.\n",
    "    \"\"\"\n",
    "    # Create Radial Basis Function (RBF) interpolators for x and y coordinates\n",
    "    rbf_x = Rbf(src_points[:, 0], src_points[:, 1], dst_points[:, 0], function='thin_plate')\n",
    "    rbf_y = Rbf(src_points[:, 0], src_points[:, 1], dst_points[:, 1], function='thin_plate')\n",
    "    \n",
    "    def transform(points):\n",
    "        new_x = rbf_x(points[:, 0], points[:, 1])\n",
    "        new_y = rbf_y(points[:, 0], points[:, 1])\n",
    "        return np.vstack([new_x, new_y]).T\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def transform_data(data_points, grid_shape_1, grid_shape_2):\n",
    "    \"\"\"\n",
    "    Apply the TPS transformation to the data points based on the grid transformation.\n",
    "    \"\"\"\n",
    "    # Perform Thin Plate Spline (TPS) transformation\n",
    "    tps_transform = thin_plate_spline_transform(grid_shape_2, grid_shape_1)\n",
    "    \n",
    "    # Apply the transformation to the data points\n",
    "    transformed_data_points = tps_transform(data_points)\n",
    "    \n",
    "    return transformed_data_points\n",
    "\n",
    "\n",
    "def transform_bone_positions(outline_dict, position_dict, common_outline, x_num=40, y_num=20, source_value=None):\n",
    "    \"\"\"\n",
    "    Transforms the bone positions from multiple datasets using Thin Plate Spline (TPS) based on the provided outlines and positions.\n",
    "    Parameters:\n",
    "        outline_dict: Dictionary containing outlines.\n",
    "        position_dict: Dictionary containing bone positions (DataFrames).\n",
    "        common_outline: The common outline (to which the other outlines will be aligned).\n",
    "        x_num: Number of vertical sections for structured grid.\n",
    "        y_num: Number of horizontal points along each vertical section.\n",
    "        source_value: If provided, filter the positions based on the specified source value.\n",
    "    Returns:\n",
    "        Dictionary containing the transformed bone positions with the 'source' column retained.\n",
    "    \"\"\"\n",
    "    transformed_dict = {}\n",
    "\n",
    "    # Create the structured grid for the common outline\n",
    "    grid_common_outline = create_structured_grid(common_outline, x_num=x_num, y_num=y_num)\n",
    "\n",
    "    # Loop through each dataset in the position_dict\n",
    "    for dataset_name, position_df in position_dict.items():\n",
    "        # Get the corresponding outline\n",
    "        outline_2 = outline_dict[dataset_name]\n",
    "        \n",
    "        # Create the structured grid for the specific dataset's outline\n",
    "        grid_outline_2 = create_structured_grid(outline_2, x_num=x_num, y_num=y_num)\n",
    "\n",
    "        # Filter the positions based on the source (if provided)\n",
    "        bone_positions_2, source_column = filter_bone_positions(position_df, source_value=source_value)\n",
    "\n",
    "        # Transform the filtered bone positions from the dataset outline to the common outline\n",
    "        transformed_bone_positions = transform_data(bone_positions_2, grid_common_outline, grid_outline_2)\n",
    "\n",
    "        # Convert the transformed positions to a DataFrame and include the source column\n",
    "        transformed_df = pd.DataFrame(transformed_bone_positions, columns=['Position.X', 'Position.Y'])\n",
    "        transformed_df['source'] = source_column  # Add the source column back\n",
    "\n",
    "        # Store the transformed DataFrame in the result dictionary\n",
    "        transformed_dict[dataset_name] = transformed_df\n",
    "\n",
    "    return transformed_dict\n",
    "\n",
    "\n",
    "def filter_bone_positions(df, source_value=None):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame for a specific source if provided and returns the Position.X, Position.Y columns as a NumPy array.\n",
    "    If source_value is None, return all positions.\n",
    "    \"\"\"\n",
    "    if source_value:\n",
    "        filtered_df = df[df['source'] == source_value]\n",
    "    else:\n",
    "        # Exclude the data with source value 'GFP'\n",
    "        filtered_df = df[df['source'] != 'GFP']\n",
    "    positions = filtered_df[['Position.X', 'Position.Y']].to_numpy()\n",
    "    return positions, filtered_df['source'].to_numpy()  # Return positions and the source column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num = 200\n",
    "y_num = 35\n",
    "# 10 mins with 200*35\n",
    "# We are using the aligned_bone_date dict instead of using the dataframes, because for bones of the same day, they have different bone outline\n",
    "# Not giving the source value, because we will apply the transformation to all the data without the data with source value 'GFP'\n",
    "transformed_ss_bones = transform_bone_positions(aligned_ss_bone_outlines_smoothed, aligned_ss_bones, common_outline, x_num=x_num, y_num=y_num)\n",
    "transformed_d5_bones = transform_bone_positions(aligned_d5_bone_outlines_smoothed, aligned_d5_bones, common_outline, x_num=x_num, y_num=y_num)\n",
    "transformed_d10_bones = transform_bone_positions(aligned_d10_bone_outlines_smoothed, aligned_d10_bones, common_outline, x_num=x_num, y_num=y_num)\n",
    "transformed_d15_bones = transform_bone_positions(aligned_d15_bone_outlines_smoothed, aligned_d15_bones, common_outline, x_num=x_num, y_num=y_num)\n",
    "transformed_d30_bones = transform_bone_positions(aligned_d30_bone_outlines_smoothed, aligned_d30_bones, common_outline, x_num=x_num, y_num=y_num)\n",
    "\n",
    "# Visualize the original and transformed bone positions for all the data sources\n",
    "transformed_ss_bones_df = pd.concat(transformed_ss_bones)\n",
    "transformed_d5_bones_df = pd.concat(transformed_d5_bones)\n",
    "transformed_d10_bones_df = pd.concat(transformed_d10_bones)\n",
    "transformed_d15_bones_df = pd.concat(transformed_d15_bones)\n",
    "transformed_d30_bones_df = pd.concat(transformed_d30_bones)\n",
    "\n",
    "# 10 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed bone positions to CSV files\n",
    "transformed_ss_bones_df.to_csv(f'{save_dir}/transformed_ss_bones_df.csv', index=False)\n",
    "transformed_d5_bones_df.to_csv(f'{save_dir}/transformed_d5_bones_df.csv', index=False)\n",
    "transformed_d10_bones_df.to_csv(f'{save_dir}/transformed_d10_bones_df.csv', index=False)\n",
    "transformed_d15_bones_df.to_csv(f'{save_dir}/transformed_d15_bones_df.csv', index=False)\n",
    "transformed_d30_bones_df.to_csv(f'{save_dir}/transformed_d30_bones_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Probability Density Map Generation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SPDM Generation (KDE Calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bone outline to generate the KDE for each source\n",
    "#  Define the KDE function for each source, with weights based on z-aggregated points\n",
    "def kde_for_source(df, bw_method='scott', bone_outline = None, binsize = 20):\n",
    "    \"\"\"\n",
    "    Apply KDE to estimate the spatial distribution in the x-y plane for each source, \n",
    "    with weights aggregated for points with the same x, y but different z.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame containing 'Position.X', 'Position.Y', 'weights', and 'source'.\n",
    "    bw_method : Bandwidth method for KDE (default is 'scott').\n",
    "    bone_outline : The outline of the bone to limit the KDE calculation within the bone.\n",
    "    binsize : The size of the bins for the grid used in the KDE calculation.\n",
    "\n",
    "    Returns:\n",
    "    kde_result : A dictionary containing the grid and KDE values for each source.\n",
    "    \"\"\"\n",
    "    kde_results = {}\n",
    "    sources = df['source'].unique()\n",
    "    sources = sources[sources != 'GFP'] # Exclude the bone for the KDE calculation\n",
    "\n",
    "    for source in sources:\n",
    "        # Filter data for the current source\n",
    "        source_data = df[df['source'] == source]\n",
    "\n",
    "        # Group by Position.X and Position.Y, summing weights (or using counts as weights if no weights are given)\n",
    "        if 'weights' in source_data.columns:\n",
    "            source_data_agg = source_data.groupby(['Position.X', 'Position.Y'])['weights'].sum().reset_index()\n",
    "        else:\n",
    "            # If no weights are provided, use the count of occurrences as weights\n",
    "            source_data_agg = source_data.groupby(['Position.X', 'Position.Y']).size().reset_index(name='weights')\n",
    "\n",
    "        # Get the x and y values and aggregated weights\n",
    "        x_vals = source_data_agg['Position.X']\n",
    "        y_vals = source_data_agg['Position.Y']\n",
    "        weights = source_data_agg['weights']\n",
    "        if bone_outline is None:\n",
    "            x_min, x_max = x_vals.min(), x_vals.max()\n",
    "            y_min, y_max = y_vals.min(), y_vals.max()\n",
    "        else:\n",
    "            x_min, y_min = bone_outline.min(axis=0)\n",
    "            x_max, y_max = bone_outline.max(axis=0)\n",
    "\n",
    "        xi, yi = np.linspace(x_min, x_max, int((x_max - x_min)/binsize)+1), np.linspace(y_min, y_max, int((y_max - y_min)/binsize)+1)\n",
    "        xi, yi = np.meshgrid(xi, yi)\n",
    "        grid_points = np.vstack([xi.flatten(), yi.flatten()])\n",
    "        common_grid = (xi, yi, grid_points)\n",
    "\n",
    "        # Stack the x and y data for KDE input\n",
    "        xy = np.vstack([x_vals, y_vals])\n",
    "\n",
    "        # Perform the KDE with aggregated weights\n",
    "        kde = gaussian_kde(xy, weights=weights, bw_method=bw_method)\n",
    "        kde_values = kde(grid_points).reshape(xi.shape)\n",
    "\n",
    "        # Store the results for each source\n",
    "        kde_results[source] = kde_values\n",
    "\n",
    "    return kde_results, common_grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using the common bone outline (outmost),  we use the overlap of the bone outlines for the KDE calculation\n",
    "kde_results_ss_overlap_bones, common_grid_overlap_bones = kde_for_source(transformed_ss_bones_df, binsize=20, bone_outline=common_outline)\n",
    "kde_results_d5_overlap_bones, _ = kde_for_source(transformed_d5_bones_df, binsize=20, bone_outline=common_outline)\n",
    "kde_results_d10_overlap_bones, _ = kde_for_source(transformed_d10_bones_df, binsize=20, bone_outline=common_outline)\n",
    "kde_results_d15_overlap_bones, _ = kde_for_source(transformed_d15_bones_df, binsize=20, bone_outline=common_outline)\n",
    "kde_results_d30_overlap_bones, _ = kde_for_source(transformed_d30_bones_df, binsize=20, bone_outline=common_outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{save_dir}/kde_results_ss_overlap_bones.pkl', 'wb') as f:\n",
    "    pickle.dump(kde_results_ss_overlap_bones, f)\n",
    "with open(f'{save_dir}/common_grid_overlap_bones.pkl', 'wb') as f:\n",
    "    pickle.dump(common_grid_overlap_bones, f)\n",
    "    \n",
    "with open(f'{save_dir}/kde_results_d5_overlap_bones.pkl', 'wb') as f:\n",
    "    pickle.dump(kde_results_d5_overlap_bones, f)\n",
    "with open(f'{save_dir}/kde_results_d10_overlap_bones.pkl', 'wb') as f:\n",
    "    pickle.dump(kde_results_d10_overlap_bones, f)\n",
    "with open(f'{save_dir}/kde_results_d15_overlap_bones.pkl', 'wb') as f:\n",
    "    pickle.dump(kde_results_d15_overlap_bones, f)\n",
    "with open(f'{save_dir}/kde_results_d30_overlap_bones.pkl', 'wb') as f:\n",
    "    pickle.dump(kde_results_d30_overlap_bones, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SPDM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to create mask from outline (2D)\n",
    "# TODO: do we need to change this to 3D?\n",
    "def points_in_polygon(x_points, y_points, outline):\n",
    "    path = Path(outline)\n",
    "    points = np.vstack((x_points, y_points)).T\n",
    "    return path.contains_points(points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KDE with the bone outline and optionally with the data points\n",
    "def plot_kde_results_with_df(df, kde_results, common_grid, save_dir, color_map, bone_outline, filename=None, mesh = True, scatter = False):\n",
    "    raw_sources = df['source'].unique()\n",
    "    raw_sources = raw_sources[raw_sources != 'GFP']\n",
    "    \n",
    "    # The source should align with the color_maps keys order\n",
    "\n",
    "    # Ensure that the sources are aligned with the keys of mk_color_map\n",
    "    # First, get the color map keys, excluding 'Bone'\n",
    "    color_map_keys = [key for key in color_map.keys() if key != 'GFP']\n",
    "\n",
    "    # Reorder sources to match the order of color_map_keys\n",
    "    sources = [source for source in color_map_keys if source in raw_sources]\n",
    "\n",
    "    \n",
    "\n",
    "    fig, axs = plt.subplots(len(sources), 1, figsize=(15 , 5 *len(sources)), sharex=True, sharey=True, constrained_layout=True)\n",
    "    if len(sources) == 1:\n",
    "        axs = [axs]  # since we only have one row, make it iterable\n",
    "\n",
    "    for i, source in enumerate(sources):\n",
    "        ax = axs[i]\n",
    "        source_positions = df[df['source'] == source]\n",
    "        \n",
    "        # Plot the x, y of each bone group as the background\n",
    "        if scatter:\n",
    "            ax.scatter(source_positions['Position.X'], source_positions['Position.Y'], s=1, c=color_map[source], alpha=1)\n",
    "        \n",
    "        xi, yi, _ = common_grid # TODO: use the dict to save the common grid so we can use string to index\n",
    "        zi = kde_results[source]\n",
    "        # Masked the values outside the bone outline\n",
    "        mask = points_in_polygon(xi.flatten(), yi.flatten(), bone_outline).reshape(xi.shape)\n",
    "        zi[~mask] = 0\n",
    "\n",
    "        # Filter only the non-zero values of zi for percentile calculation (i.e., exclude outside the mask)\n",
    "        zi_inside_mask = zi[mask]\n",
    "\n",
    "        # Normalize the values of zi for the full plot\n",
    "        norm = Normalize(vmin=zi_inside_mask.min(), vmax=zi_inside_mask.max())\n",
    "        normed_z = norm(zi)\n",
    "        normed_z[~mask] = 0\n",
    "        # Set the alpha transparency to 1 for test only\n",
    "        # normed_z[mask] = 1    \n",
    "        colors = np.array(plt.cm.colors.hex2color(color_map[source]))\n",
    "        rgba_colors = np.zeros((*zi.shape, 4))\n",
    "        rgba_colors[..., :3] = colors[:3]  # RGB values\n",
    "        rgba_colors[..., -1] = normed_z  # Alpha transparency \n",
    "        \n",
    "        # rgba_colors[zi < np.percentile(zi, 80)] = 0\n",
    "        \n",
    "        ax.set_title(source)\n",
    "        xlim_min, ylim_min = bone_outline.min(axis=0)\n",
    "        xlim_max, ylim_max = bone_outline.max(axis=0)\n",
    "        # Convert them into integers\n",
    "        xlim_min, xlim_max = int(xlim_min)-500, int(xlim_max)+500\n",
    "        ylim_min, ylim_max = int(ylim_min)-300, int(ylim_max)+300\n",
    "        \n",
    "        ax.set_xlim(xlim_min, xlim_max)\n",
    "        ax.set_ylim(ylim_min, ylim_max)\n",
    "        ax.set_xlabel('Position.X')\n",
    "        ax.set_ylabel('Position.Y')\n",
    "        if mesh:\n",
    "            ax.pcolormesh(xi, yi, rgba_colors, shading='auto', rasterized=True)\n",
    "        \n",
    "        percentiles = np.arange(0, 81, 20)\n",
    "        contour_levels = np.unique(np.percentile(norm(zi_inside_mask), percentiles))\n",
    "        colors = color_map[source]\n",
    "        if len(contour_levels) > 1:\n",
    "            # print(contour_levels)\n",
    "            # With changing linewidths, from the largest to the smallest\n",
    "            # linewidths = np.linspace(0.5, 10, len(contour_levels))\n",
    "            contour = ax.contour(xi, yi, normed_z, levels=contour_levels, linewidths=1, colors='black', alpha=0.5)\n",
    "            \n",
    "\n",
    "        # fmt = {level: f'{perc}%' for level, perc in zip(contour_levels, percentiles)}\n",
    "        # ax.clabel(contour, contour_levels, inline=True, fmt=fmt, fontsize=8)\n",
    "\n",
    "        ax.plot(bone_outline[:,0], bone_outline[:, 1], color='black')\n",
    "        # TODO: optional\n",
    "        # Find the peak in the kde result\n",
    "        max_idx = np.unravel_index(np.argmax(zi), zi.shape)  # Index of maximum value in zi\n",
    "        peak_x, peak_y = xi[max_idx], yi[max_idx]  # Get the coordinates of the peak\n",
    "\n",
    "        # Plot the peak as a marker\n",
    "        ax.plot(peak_x, peak_y, 's', markersize=5, label='Peak', color = 'black')  # Black square marker showing the global peak\n",
    "\n",
    "    # Save the figure for the current bone group in the corresponding directory\n",
    "    fig.suptitle(f'Spatial Distribution Estimation of {filename}', fontsize=16)\n",
    "    if save_dir:\n",
    "        if filename:\n",
    "            fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{filename}.pdf')\n",
    "            # fig_filename_pdf = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{filename}.pdf')\n",
    "        else:\n",
    "            fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{sources}.pdf') \n",
    "            # fig_filename_pdf = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{sources}.pdf')\n",
    "        # Save it as pdf file\n",
    "        fig.savefig(fig_filename, dpi=300, bbox_inches='tight')\n",
    "        # Increase the maximum image size limit\n",
    "        # Image.MAX_IMAGE_PIXELS = None\n",
    "        \n",
    "        # image = Image.open(fig_filename)\n",
    "        # image.convert('RGB').save(fig_filename_pdf)\n",
    "    #plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KDE of MKs and Adipos with the bone outline\n",
    "def plot_kde_results_MK_Adipos(kde_results, common_grid, save_dir, color_map, bone_outline, filename=None, mesh = True):\n",
    "    sources = ['MKs','Adipos']\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15 , 5))\n",
    "    legend_patches = []\n",
    "    for i, source in enumerate(sources):\n",
    "        # if source == 'Adipos':\n",
    "        #     continue\n",
    "        # source_positions = df[df['source'] == source]\n",
    "        # Plot the x, y of each bone group as the background\n",
    "        # ax.scatter(source_positions['Position.X'], source_positions['Position.Y'], s=3, c=color_map[source], alpha=0.3)\n",
    "        \n",
    "        xi, yi, _ = common_grid # TODO: use the dict to save the common grid so we can use string to index\n",
    "        zi = kde_results[source]\n",
    "        # Masked the values outside the bone outline\n",
    "        mask = points_in_polygon(xi.flatten(), yi.flatten(), bone_outline).reshape(xi.shape)\n",
    "        zi[~mask] = 0\n",
    "        \n",
    "        # Make sure the values sum to 1\n",
    "        # zi = zi / zi.sum()\n",
    "        \n",
    "        # Filter only the non-zero values of zi for percentile calculation (i.e., exclude outside the mask)\n",
    "        zi_inside_mask = zi[mask]\n",
    "\n",
    "        # Normalize the values of zi for the full plot\n",
    "        norm = Normalize(vmin=zi_inside_mask.min(), vmax=zi_inside_mask.max())\n",
    "        normed_z = norm(zi)\n",
    "        normed_z[~mask] = 0\n",
    "        \n",
    "        # Set all no-zores values to 1\n",
    "        # zi[zi > np.percentile(zi, 80)] = 1\n",
    "\n",
    "        legend_patches.append(Patch(color=color_map[source], label=source, alpha = 0.5))\n",
    "\n",
    "        colors = np.array(plt.cm.colors.hex2color(color_map[source]))\n",
    "        rgba_colors = np.zeros((*zi.shape, 4))\n",
    "        rgba_colors[..., :3] = colors[:3]  # RGB values\n",
    "        \n",
    "        # set the transparency into 5 levels based on normed_z\n",
    "        # TODO\n",
    "        # alpha_levels = np.linspace(0.1, 0.5, 5)\n",
    "        # for i, alpha in enumerate(alpha_levels):\n",
    "        #     rgba_colors[normed_z > np.percentile(zi_inside_mask, 20*i)] = [*colors[:3], alpha]\n",
    "        rgba_colors[..., -1] = normed_z  # Alpha transparency\n",
    "        # rgba_colors[normed_z < np.percentile(normed_z, 80)] = 0\n",
    "        \n",
    "\n",
    "        if mesh:\n",
    "            ax.pcolormesh(xi, yi, rgba_colors, shading='auto', rasterized=True)\n",
    "\n",
    "        \n",
    "        percentiles = np.arange(0, 81, 20) # for 5 levels and if we want 10 levels we need to use np.arange(10, 101, 10)\n",
    "        contour_levels = np.percentile(norm(zi_inside_mask), percentiles)\n",
    "        # contour_levels = np.unique(np.percentile(norm(zi), percentiles))\n",
    "        # Print the calculated contour levels\n",
    "        print(\"Calculated Contour Levels:\", contour_levels)\n",
    "        rgba_colors[normed_z < contour_levels[-1]] = 0\n",
    "        colors = color_map[source]\n",
    "        if len(contour_levels) > 1:\n",
    "            # With changing linewidths, from the largest to the smallest\n",
    "            # linewidths = np.linspace(1, 5, len(contour_levels))\n",
    "            # contour = ax.contour(xi, yi, normed_z, levels=contour_levels, linewidths=linewidths, colors=colors, alpha=0.5)\n",
    "\n",
    "            # Only show the last two levels\n",
    "            # linewidths = np.linspace(1, 5, len(contour_levels))\n",
    "            linewidths = [2,4]\n",
    "            contour = ax.contour(xi, yi, normed_z, levels=contour_levels[-2:], linewidths=linewidths, colors=colors, alpha=0.5)\n",
    "            \n",
    "            # fmt = {level: f'{100- perc}%' for level, perc in zip(contour_levels[-2:], percentiles[-2:])}\n",
    "            # ax.clabel(contour, contour_levels[-2:], inline=True, fmt=fmt, fontsize=8, colors=colors)\n",
    "\n",
    "    xlim_min, ylim_min = bone_outline.min(axis=0)\n",
    "    xlim_max, ylim_max = bone_outline.max(axis=0)\n",
    "    # Convert them into integers\n",
    "    xlim_min, xlim_max = int(xlim_min)-500, int(xlim_max)+500\n",
    "    ylim_min, ylim_max = int(ylim_min)-300, int(ylim_max)+300\n",
    "    ax.set_title(filename)\n",
    "    ax.set_xlim(xlim_min, xlim_max)\n",
    "    ax.set_ylim(ylim_min, ylim_max)\n",
    "    ax.set_xlabel('Position.X')\n",
    "    ax.set_ylabel('Position.Y')\n",
    "    ax.plot(bone_outline[:,0], bone_outline[:, 1], color='black')\n",
    "    ax.legend(handles = legend_patches, loc='upper right')\n",
    "    # Save the figure for the current bone group in the corresponding directory\n",
    "    fig.suptitle(f'Spatial Probability Density Map', fontsize=16)\n",
    "    if save_dir:\n",
    "        if filename:\n",
    "            fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{filename}.pdf')\n",
    "            fig_filename_pdf = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{filename}.pdf')\n",
    "        else:\n",
    "            fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{sources}.pdf') \n",
    "            fig_filename_pdf = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_KDE_{sources}.pdf')\n",
    "        fig.savefig(fig_filename, dpi=300)\n",
    "        # Increase the maximum image size limit\n",
    "        # Image.MAX_IMAGE_PIXELS = None\n",
    "        \n",
    "        # image = Image.open(fig_filename)\n",
    "        # image.convert('RGB').save(fig_filename_pdf)\n",
    "    #plt.close(fig)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1D_kde(kde_results, common_grid, save_dir, color_map, bone_outline, sources=['MKs', 'Adipos'], filename=None):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "    \n",
    "    for source in sources:\n",
    "        xi, yi, _ = common_grid  # Get the common grid (xi for X, yi for Y)\n",
    "        zi = kde_results[source]  # Get the KDE results for the current source\n",
    "        \n",
    "        # Mask values outside the bone outline\n",
    "        mask = points_in_polygon(xi.flatten(), yi.flatten(), bone_outline).reshape(xi.shape)\n",
    "        zi[~mask] = 0\n",
    "        \n",
    "        # Normalize the KDE results to ensure they sum to 1\n",
    "        zi = zi / zi.sum()\n",
    "        \n",
    "        # Sum along the y-axis (rows) to project the KDE along the x-axis\n",
    "        summed_zi_along_y = zi.sum(axis=0)  # Summing along rows gives us the KDE projection along X\n",
    "        # Smooth the summed_zi_along_y with Gaussian filter\n",
    "        smoothed_summed_zi = gaussian_filter1d(summed_zi_along_y, sigma=10)  # Adjust sigma as needed for smoothing\n",
    "    \n",
    "        # Plot the summed values as a 1D curve\n",
    "\n",
    "        name_label = source\n",
    "\n",
    "        # ax.plot(xi[0, :], summed_zi_along_y, label=f'{name_label} (original)', color=color_map[source], linewidth=2)\n",
    "    \n",
    "        # Plot the smoothed values as a dashed line in black\n",
    "        ax.plot(xi[0, :], smoothed_summed_zi, label=f'{name_label}', color=color_map[source], linewidth=2) # linestyle='--', \n",
    "    # Set plot title, labels, and legend\n",
    "    if filename:\n",
    "        name_end = filename.split('_')[-1]\n",
    "        ax.set_title(f'Marginal Distribution of {','.join(sources)} {name_end}', fontsize=16)\n",
    "    else:\n",
    "        ax.set_title('Marginal Distribution of MKs and Adipos', fontsize=16)\n",
    "    ax.set_xlabel('Position.X', fontsize=14)\n",
    "    ax.set_ylabel('Marginized Probability Density', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "    # Set the min-max for y-axis\n",
    "    # Get the current y-axis limits\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "\n",
    "    # Set the y-ticks only at the min and max\n",
    "    ax.set_yticks([y_min, y_max])\n",
    "\n",
    "    # Set the y-tick labels to display min and max values\n",
    "    ax.set_yticklabels(['min', 'max'])\n",
    "    \n",
    "    # Optionally, save the figure\n",
    "    if save_dir:\n",
    "        if filename:\n",
    "            fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_1D_KDE_min_max_{filename}.pdf')\n",
    "        else:\n",
    "            fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_1D_KDE_MKs_Adipos.pdf')\n",
    "        fig.savefig(fig_filename, dpi=300)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the KDE results per bone with the corresponding outline\n",
    "plot_kde_results_with_df(transformed_ss_bones_df, kde_results_ss_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'd0')\n",
    "plot_kde_results_with_df(transformed_d5_bones_df, kde_results_d5_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'd5')\n",
    "plot_kde_results_with_df(transformed_d10_bones_df, kde_results_d10_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'd10')\n",
    "plot_kde_results_with_df(transformed_d15_bones_df, kde_results_d15_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'd15')\n",
    "plot_kde_results_with_df(transformed_d30_bones_df, kde_results_d30_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'd30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDE results per bone with the corresponding outline\n",
    "plot_kde_results_with_df(transformed_ss_bones_df, kde_results_ss_overlap_bones, common_grid_overlap_bones, save_dir, hscs_color_map, common_outline, 'hsc_rd_d0', scatter=True)\n",
    "plot_kde_results_with_df(transformed_d5_bones_df, kde_results_d5_overlap_bones, common_grid_overlap_bones, save_dir, hscs_color_map, common_outline, 'hsc_rd_d5', scatter=True)\n",
    "plot_kde_results_with_df(transformed_d10_bones_df, kde_results_d10_overlap_bones, common_grid_overlap_bones, save_dir, hscs_color_map, common_outline, 'hsc_rd_d10', scatter=True)\n",
    "plot_kde_results_with_df(transformed_d15_bones_df, kde_results_d15_overlap_bones, common_grid_overlap_bones, save_dir, hscs_color_map, common_outline, 'hsc_rd_d15', scatter=True)\n",
    "plot_kde_results_with_df(transformed_d30_bones_df, kde_results_d30_overlap_bones, common_grid_overlap_bones, save_dir, hscs_color_map, common_outline, 'hsc_rd_d30', scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1D_kde(kde_results_ss_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, filename='mk_adipos_d0')\n",
    "plot_1D_kde(kde_results_d5_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, filename='mk_adipos_d5')\n",
    "plot_1D_kde(kde_results_d10_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, filename='mk_adipos_d10')\n",
    "plot_1D_kde(kde_results_d15_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, filename='mk_adipos_d15')\n",
    "plot_1D_kde(kde_results_d30_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, filename='mk_adipos_d30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['MKs', 'Adipos', 'Sinusoids', 'MSCs', 'Neurons', 'Arteries']\n",
    "plot_1D_kde(kde_results_ss_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, sources = sources, filename='all_d0')\n",
    "plot_1D_kde(kde_results_d5_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, sources = sources, filename='all_d5')\n",
    "plot_1D_kde(kde_results_d10_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, sources = sources, filename='all_d10')\n",
    "plot_1D_kde(kde_results_d15_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, sources = sources, filename='all_d15')\n",
    "plot_1D_kde(kde_results_d30_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, sources = sources, filename='all_d30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde_results_MK_Adipos(kde_results_ss_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'mk_adipos_d0')\n",
    "plot_kde_results_MK_Adipos(kde_results_d5_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'mk_adipos_d5')\n",
    "plot_kde_results_MK_Adipos(kde_results_d10_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'mk_adipos_d10')\n",
    "plot_kde_results_MK_Adipos(kde_results_d15_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'mk_adipos_d15')\n",
    "plot_kde_results_MK_Adipos(kde_results_d30_overlap_bones, common_grid_overlap_bones, save_dir, mk_color_map, common_outline, 'mk_adipos_d30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Clustering and Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_kde_features(conditions, density_features, interested_source, common_grid, bone_outline):\n",
    "    \"\"\"\n",
    "    Calculate KDE features for HSCs or RDs using nearest interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    - conditions: List of tuples, where each tuple contains (condition_name, dataframe, kde_results_feature).\n",
    "    - density_features: List of feature names (e.g., ['MKs', 'Adipos', 'Sinusoids', ...]).\n",
    "    - interested_source: The source to calculate KDE for ('HSCs' or 'RDs').\n",
    "    - common_grid: Tuple containing xi, yi, and other grid parameters.\n",
    "    - bone_outline: Outline for masking KDE values outside the bone area.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with KDE values for the specified source across all conditions.\n",
    "    \"\"\"\n",
    "    xi, yi, _ = common_grid\n",
    "    xi_flattened = xi.flatten()\n",
    "    yi_flattened = yi.flatten()\n",
    "\n",
    "    # Generate the bone mask once since it's the same for all conditions\n",
    "    mask = points_in_polygon(xi_flattened, yi_flattened, bone_outline).reshape(xi.shape)\n",
    "\n",
    "    # Prepare a list to collect DataFrames for each condition\n",
    "    kde_feature_list = []\n",
    "\n",
    "    # Loop over each condition and process KDE\n",
    "    for condition_name, df, kde_results_feature in conditions:\n",
    "        \n",
    "        # Filter the data based on the interested source (HSCs or RDs)\n",
    "        positions = df[df['source'] == interested_source][['Position.X', 'Position.Y']].values\n",
    "        \n",
    "        # Prepare a dictionary to store KDE features for the current condition\n",
    "        kde_values_dict = {'Position.X': positions[:, 0], 'Position.Y': positions[:, 1], 'condition': condition_name}\n",
    "        \n",
    "        # Loop over each density feature and interpolate KDE values\n",
    "        for source in density_features:\n",
    "            zi = kde_results_feature[source].copy()\n",
    "            zi[~mask] = 0  # Mask KDE values outside the bone outline\n",
    "            \n",
    "            # Normalize the KDE values within the mask to the range 0-1\n",
    "            zi_inside_mask = zi[mask]\n",
    "            norm = Normalize(vmin=zi_inside_mask.min(), vmax=zi_inside_mask.max())\n",
    "            normed_z = norm(zi)\n",
    "            normed_z[~mask] = 0\n",
    "            \n",
    "            # Interpolate KDE values for the interested source positions\n",
    "            kde_values = griddata(\n",
    "                (xi_flattened, yi_flattened),\n",
    "                normed_z.flatten(),\n",
    "                (positions[:, 0], positions[:, 1]),\n",
    "                method='nearest',\n",
    "                fill_value=0\n",
    "            )\n",
    "            \n",
    "            # Add the interpolated KDE values to the dictionary\n",
    "            kde_values_dict[source] = kde_values\n",
    "        \n",
    "        # Append the dictionary as a DataFrame for the current condition\n",
    "        kde_feature_list.append(pd.DataFrame(kde_values_dict))\n",
    "    \n",
    "    # Concatenate all condition DataFrames into a single DataFrame\n",
    "    return pd.concat(kde_feature_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_pos_kde = [\n",
    "    (\"d0\", transformed_ss_bones_df, kde_results_ss_overlap_bones),\n",
    "    (\"d5\", transformed_d5_bones_df, kde_results_d5_overlap_bones),\n",
    "    (\"d10\", transformed_d10_bones_df, kde_results_d10_overlap_bones),\n",
    "    (\"d15\", transformed_d15_bones_df, kde_results_d15_overlap_bones),\n",
    "    (\"d30\", transformed_d30_bones_df, kde_results_d30_overlap_bones)\n",
    "]\n",
    "\n",
    "density_features = ['MKs', 'Adipos', 'Sinusoids', 'MSCs', 'Neurons', 'Arteries']\n",
    "\n",
    "kde_values_for_hscs_nearest = calculate_kde_features(\n",
    "    conditions=conditions_pos_kde,\n",
    "    density_features=density_features,\n",
    "    interested_source=\"HSCs\",\n",
    "    common_grid=common_grid_overlap_bones,\n",
    "    bone_outline=common_outline\n",
    ")\n",
    "kde_values_for_rds_nearest = calculate_kde_features(\n",
    "    conditions=conditions_pos_kde,\n",
    "    density_features=density_features,\n",
    "    interested_source=\"RDs\",\n",
    "    common_grid=common_grid_overlap_bones,\n",
    "    bone_outline=common_outline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Heatmap Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering on the KDE values for HSCs\n",
    "# Select the optimal number of clusters based on the Silhouette Score\n",
    "hscs_features = kde_values_for_hscs_nearest[density_features]\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "linkage_matrix_hscs = linkage(hscs_features, method='ward')\n",
    "\n",
    "# Range of cluster numbers to evaluate\n",
    "cluster_numbers = range(2,20)  # You can adjust this range based on your hscs_features\n",
    "silhouette_scores = []\n",
    "\n",
    "# Calculate the Silhouette Score for each number of clusters\n",
    "for n_clusters in cluster_numbers:\n",
    "    # Assign clusters based on the number of clusters (cutting the dendrogram)\n",
    "    cluster_labels = fcluster(linkage_matrix_hscs, t=n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Calculate the Silhouette Score for the current number of clusters\n",
    "    score = silhouette_score(hscs_features, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Find the optimal number of clusters based on the highest Silhouette Score\n",
    "optimal_clusters = cluster_numbers[np.argmax(silhouette_scores)]\n",
    "print(f'Optimal number of clusters: {optimal_clusters} with Silhouette Score: {max(silhouette_scores):.3f}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cluster_numbers, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different Cluster Numbers')\n",
    "plt.grid(True)\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_HSCs_silhouette.pdf')\n",
    "plt.savefig(fig_filename, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the hierarchical clustering heatmap of HSCs based on KDE values\n",
    "# Assign clusters based on the optimal number of clusters (8 in this case)\n",
    "kde_values_for_hscs_nearest['cluster'] = fcluster(linkage_matrix_hscs, t=optimal_clusters, criterion='maxclust')\n",
    "\n",
    "# Add a color mapping for the conditions\n",
    "condition_colors = {\n",
    "    'd0': '#FB9A99',  # Pink\n",
    "    'd5': '#CAB2D6',  # Lavender\n",
    "    'd10': '#FF7F00', # - Orange\n",
    "    'd15': '#A6CEE3', # - Light Blue\n",
    "    'd30': '#33A02C' # - Green\n",
    "}\n",
    "row_colors_condition = kde_values_for_hscs_nearest['condition'].map(condition_colors)\n",
    "\n",
    "# Define custom colors for each cluster\n",
    "cluster_colors = {\n",
    "    1: '#B15928', # - Brown\n",
    "    2: '#6A3D9A', # - Purple\n",
    "    3: '#1F78B4'   # Blue\n",
    "}\n",
    "\n",
    "# Map these colors to the 'cluster' column in your DataFrame\n",
    "row_colors_cluster = kde_values_for_hscs_nearest['cluster'].map(cluster_colors)\n",
    "\n",
    "# Combine row colors into a DataFrame\n",
    "row_colors_df = pd.DataFrame({\n",
    "    'Condition': row_colors_condition,  # Existing condition color mapping\n",
    "    'Cluster': row_colors_cluster       # Custom cluster color mapping\n",
    "})\n",
    "\n",
    "\n",
    "# Create a larger figure to accommodate both clustermap and legend\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "min_value = hscs_features.values.min()\n",
    "max_value = hscs_features.values.max()\n",
    "# Create the clustermap with the row colors\n",
    "g = sns.clustermap(\n",
    "    hscs_features,\n",
    "    row_linkage=linkage_matrix_hscs,\n",
    "    col_cluster=False,  # Do not cluster the columns\n",
    "    cmap='coolwarm',\n",
    "    method = 'ward',\n",
    "    figsize=(9, 12),\n",
    "    row_colors=row_colors_df,  # Add row colors for conditions and clusters\n",
    "    # standard_scale= 1,\n",
    "    xticklabels=True,\n",
    "    yticklabels=False,  # Hide row indices\n",
    "    cbar_pos=(1.2, 0.7, 0.07, 0.18),  # Adjust x position of the color bar\n",
    "    dendrogram_ratio=0.1, colors_ratio=0.02,\n",
    "    vmin=min_value,  # Set the minimum color bar value\n",
    "    vmax=max_value   # Set the maximum color bar value to the max of the data\n",
    ")\n",
    "\n",
    "# Set the color bar ticks and labels\n",
    "colorbar = g.cax  # Access the color bar\n",
    "colorbar.set_yticks([min_value, 0.50, max_value])  # Set color bar ticks at min and max values\n",
    "colorbar.set_yticklabels([f'{min_value:.2f}',f'{0.50:.2f}', f'{max_value:.2f}'])  # Annotate color bar with min and max\n",
    "\n",
    "\n",
    "# Add legends for the row colors (condition and cluster)\n",
    "legend_handles = [\n",
    "    mpatches.Patch(color=color, label=label) for label, color in condition_colors.items()\n",
    "] + [\n",
    "    mpatches.Patch(color=color, label=f'Cluster {i+1}') for i, color in enumerate(cluster_colors.values())\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Condition and Cluster',\n",
    "    bbox_to_anchor=(1.2, -1),  # Align with color bar x position\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "# Show the plot\n",
    "plt.suptitle('Hierarchical Clustering Heatmap of HSCs Based on KDE Values of Different Populations', fontsize=16, y=1)\n",
    "save_dir = f'results/{datetime.today().strftime(\"%y%m%d\")}_YL_PositionKDE'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_HSCs_heatmaps.pdf')\n",
    "g.savefig(fig_filename, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the hierarchical clustering heatmap of HSCs based on KDE values\n",
    "\n",
    "rds_features = kde_values_for_rds_nearest[density_features]\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "linkage_matrix_rds = linkage(rds_features, method='ward')\n",
    "\n",
    "# Assign clusters based on the optimal number of clusters (8 in this case)\n",
    "kde_values_for_rds_nearest['cluster'] = fcluster(linkage_matrix_rds, t=optimal_clusters, criterion='maxclust')\n",
    "\n",
    "row_colors_condition = kde_values_for_rds_nearest['condition'].map(condition_colors)\n",
    "\n",
    "# Map these colors to the 'cluster' column in your DataFrame\n",
    "row_colors_cluster = kde_values_for_rds_nearest['cluster'].map(cluster_colors)\n",
    "\n",
    "# Combine row colors into a DataFrame\n",
    "row_colors_df = pd.DataFrame({\n",
    "    'Condition': row_colors_condition,  # Existing condition color mapping\n",
    "    'Cluster': row_colors_cluster       # Custom cluster color mapping\n",
    "})\n",
    "\n",
    "\n",
    "# Create a larger figure to accommodate both clustermap and legend\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "min_value = rds_features.values.min()\n",
    "max_value = rds_features.values.max()\n",
    "# Create the clustermap with the row colors\n",
    "g = sns.clustermap(\n",
    "    rds_features,\n",
    "    row_linkage=linkage_matrix_rds,\n",
    "    col_cluster=False,  # Do not cluster the columns\n",
    "    cmap='coolwarm',\n",
    "    method = 'ward',\n",
    "    figsize=(9, 12),\n",
    "    row_colors=row_colors_df,  # Add row colors for conditions and clusters\n",
    "    # standard_scale= 1,\n",
    "    xticklabels=True,\n",
    "    yticklabels=False,  # Hide row indices\n",
    "    cbar_pos=(1.2, 0.7, 0.07, 0.18),  # Adjust x position of the color bar\n",
    "    dendrogram_ratio=0.1, colors_ratio=0.02,\n",
    "    vmin=min_value,  # Set the minimum color bar value\n",
    "    vmax=max_value   # Set the maximum color bar value to the max of the data\n",
    ")\n",
    "\n",
    "# Set the color bar ticks and labels\n",
    "colorbar = g.cax  # Access the color bar\n",
    "colorbar.set_yticks([min_value, 0.50, max_value])  # Set color bar ticks at min and max values\n",
    "colorbar.set_yticklabels([f'{min_value:.2f}',f'{0.50:.2f}', f'{max_value:.2f}'])  # Annotate color bar with min and max\n",
    "\n",
    "# Add legends for the row colors (condition and cluster)\n",
    "legend_handles = [\n",
    "    mpatches.Patch(color=color, label=label) for label, color in condition_colors.items()\n",
    "] + [\n",
    "    mpatches.Patch(color=color, label=f'Cluster {i+1}') for i, color in enumerate(cluster_colors.values())\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Condition and Cluster',\n",
    "    bbox_to_anchor=(1.2, -1),  # Align with color bar x position\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "# Show the plot\n",
    "plt.suptitle('Hierarchical Clustering Heatmap of RDs Based on KDE Values of Different Populations', fontsize=16, y=1)\n",
    "save_dir = f'results/{datetime.today().strftime(\"%y%m%d\")}_YL_PositionKDE'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_rd_heatmaps.pdf')\n",
    "g.savefig(fig_filename, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate HSCs and RDs DataFrames\n",
    "combined_kde_values = pd.concat([kde_values_for_hscs_nearest, kde_values_for_rds_nearest], ignore_index=True)\n",
    "\n",
    "# Add a 'group' column to distinguish HSCs and RDs\n",
    "combined_kde_values['group'] = combined_kde_values.apply(lambda x: 'HSCs' if x.name < len(kde_values_for_hscs_nearest) else 'RDs', axis=1)\n",
    "\n",
    "# Extract KDE feature columns for both HSCs and RDs\n",
    "hscs_features = kde_values_for_hscs_nearest[density_features]\n",
    "rds_features = kde_values_for_rds_nearest[density_features]\n",
    "\n",
    "# Combine HSCs and RDs KDE features into a single DataFrame\n",
    "combined_features = pd.concat([hscs_features, rds_features], ignore_index=True)\n",
    "\n",
    "# Perform hierarchical clustering on the combined features using the 'ward' method\n",
    "linkage_matrix_combined = linkage(combined_features, method='ward')\n",
    "\n",
    "# Range of cluster numbers to evaluate\n",
    "cluster_numbers = range(2,11)  # You can adjust this range based on your hscs_features\n",
    "silhouette_scores = []\n",
    "\n",
    "# Calculate the Silhouette Score for each number of clusters\n",
    "for n_clusters in cluster_numbers:\n",
    "    # Assign clusters baon the number of clusters (cutting the dendrogram)\n",
    "    cluster_labels = fcluster(linkage_matrix_combined, t=n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Calculate the Silhouette Score for the current number of clusters\n",
    "    score = silhouette_score(combined_features, cluster_labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Find the optimal number of clusters based on the highest Silhouette Score\n",
    "optimal_clusters_combined = cluster_numbers[np.argmax(silhouette_scores)]\n",
    "print(f'Optimal number of clusters: {optimal_clusters_combined} with Silhouette Score: {max(silhouette_scores):.3f}')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cluster_numbers, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Different Cluster Numbers')\n",
    "plt.grid(True)\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_silhouette_combined.pdf')\n",
    "plt.savefig(fig_filename, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each cluster\n",
    "combined_kde_values['cluster'] = fcluster(linkage_matrix_combined, t=optimal_clusters_combined, criterion='maxclust')\n",
    "row_colors_combined_cluster = combined_kde_values['cluster'].map(cluster_colors)\n",
    "row_colors_combined_condition = combined_kde_values['condition'].map(condition_colors)\n",
    "row_colors_combined_group = combined_kde_values['group'].map({'HSCs': 'gray', 'RDs': 'black'})\n",
    "\n",
    "# Combine row colors into a DataFrame with condition, cluster, and group colors\n",
    "row_colors_df_combined = pd.DataFrame({\n",
    "    'Condition': row_colors_combined_condition,\n",
    "    'Cluster': combined_kde_values['cluster'].map(cluster_colors),\n",
    "    'Group': row_colors_combined_group\n",
    "})\n",
    "\n",
    "# Calculate the minimum and maximum value across all feature columns\n",
    "min_value = combined_features.values.min()\n",
    "max_value = combined_features.values.max()\n",
    "\n",
    "# Plot the clustermap with custom color bar limits\n",
    "g = sns.clustermap(\n",
    "    combined_features,\n",
    "    row_linkage=linkage_matrix_combined,\n",
    "    col_cluster=False,\n",
    "    cmap='coolwarm',\n",
    "    method='ward',\n",
    "    figsize=(9, 12),\n",
    "    row_colors=row_colors_df_combined,\n",
    "    xticklabels=True,\n",
    "    yticklabels=False,\n",
    "    cbar_pos=(1.2, 0.7, 0.07, 0.18),\n",
    "    dendrogram_ratio=0.1,\n",
    "    colors_ratio=0.02,\n",
    "    vmin=min_value,  # Set the minimum color bar value\n",
    "    vmax=max_value   # Set the maximum color bar value to the max of the data\n",
    ")\n",
    "\n",
    "# Set the color bar ticks and labels\n",
    "colorbar = g.cax  # Access the color bar\n",
    "colorbar.set_yticks([min_value, 0.50, max_value])  # Set color bar ticks at min and max values\n",
    "colorbar.set_yticklabels([f'{min_value:.2f}',f'{0.50:.2f}', f'{max_value:.2f}'])  # Annotate color bar with min and max\n",
    "\n",
    "# Add legends for conditions, clusters, and groups\n",
    "legend_handles = [\n",
    "    mpatches.Patch(color=color, label=label) for label, color in condition_colors.items()\n",
    "] + [\n",
    "    mpatches.Patch(color=color, label=f'Cluster {i+1}') for i, color in enumerate(cluster_colors.values())\n",
    "] + [\n",
    "    mpatches.Patch(color='gray', label='HSCs'), mpatches.Patch(color='black', label='RDs')\n",
    "]\n",
    "\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    title='Condition, Cluster, and Group',\n",
    "    bbox_to_anchor=(1.2, -1)\n",
    ")\n",
    "\n",
    "# Show the plot with a title\n",
    "plt.suptitle('Hierarchical Clustering Heatmap of HSCs and RDs Based on KDE Values', fontsize=16, y=1)\n",
    "save_dir = f'results/{datetime.today().strftime(\"%y%m%d\")}_YL_PositionKDE'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_HSCs_RDs_heatmaps.pdf')\n",
    "g.savefig(fig_filename, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the clusters of the combined KDE values based on the optimal number of clusters\n",
    "combined_kde_values['cluster'] = fcluster(linkage_matrix_combined, t=optimal_clusters, criterion='maxclust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_combined_kde_values = combined_kde_values[combined_kde_values['group'] == 'HSCs']\n",
    "# Define the desired order for the x-axis categories\n",
    "desired_order = ['d0', 'd5', 'd10', 'd15', 'd30']\n",
    "\n",
    "# Group by condition and cluster to count the number of HSCs in each cluster per condition\n",
    "cluster_counts = filtered_combined_kde_values.groupby(['condition', 'cluster']).size().unstack(fill_value=0)\n",
    "# Reindex the DataFrame to ensure the x-axis follows the specified order\n",
    "cluster_counts = cluster_counts.reindex(desired_order)\n",
    "\n",
    "# Extract custom colors for each cluster in the order of clusters\n",
    "color_list = [cluster_colors[cluster] for cluster in sorted(cluster_colors.keys())]\n",
    "\n",
    "# Plot the stacked bar chart with custom colors\n",
    "cluster_counts.plot(kind='bar', stacked=True, color=color_list, figsize=(12, 8))\n",
    "\n",
    "# Labeling the plot\n",
    "plt.title('Cluster Composition by Condition', fontsize=16)\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Number of HSCs')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_HSCs_barplot_counts.pdf')\n",
    "plt.savefig(fig_filename, dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert counts to percentages\n",
    "cluster_percentages = cluster_counts.div(cluster_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the stacked bar chart with increased bar width\n",
    "ax = cluster_percentages.plot(kind='bar', stacked=True, color=color_list, figsize=(12, 8), width=1)\n",
    "\n",
    "# Number of conditions (categories) on the x-axis\n",
    "num_conditions = len(cluster_percentages)\n",
    "# Labeling the plot\n",
    "ax.set_title('Cluster Composition by Condition (Percentage)', fontsize=16)\n",
    "ax.set_xlabel('Condition')\n",
    "ax.set_ylabel('Percentage of HSCs')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlim([-0.5, num_conditions - 0.5]) \n",
    "\n",
    "\n",
    "ax.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_HSCs_bar_percentage.pdf')\n",
    "plt.savefig(fig_filename, dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_combined_kde_values = combined_kde_values[combined_kde_values['group'] == 'RDs']\n",
    "# Group by condition and cluster to count the number of HSCs in each cluster per condition\n",
    "cluster_counts = filtered_combined_kde_values.groupby(['condition', 'cluster']).size().unstack(fill_value=0)\n",
    "cluster_counts = cluster_counts.reindex(desired_order)\n",
    "# Extract custom colors for each cluster in the order of clusters\n",
    "color_list = [cluster_colors[cluster] for cluster in sorted(cluster_colors.keys())]\n",
    "\n",
    "# Plot the stacked bar chart with custom colors\n",
    "cluster_counts.plot(kind='bar', stacked=True, color=color_list, figsize=(12, 8))\n",
    "\n",
    "# Labeling the plot\n",
    "plt.title('Cluster Composition by Condition', fontsize=16)\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Number of HSCs')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_RDs_barplot_counts.pdf')\n",
    "plt.savefig(fig_filename, dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert counts to percentages\n",
    "cluster_percentages = cluster_counts.div(cluster_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the stacked bar chart with increased bar width\n",
    "ax = cluster_percentages.plot(kind='bar', stacked=True, color=color_list, figsize=(12, 8), width=1)\n",
    "\n",
    "# Number of conditions (categories) on the x-axis\n",
    "num_conditions = len(cluster_percentages)\n",
    "# Labeling the plot\n",
    "ax.set_title('Cluster Composition by Condition (Percentage)', fontsize=16)\n",
    "ax.set_xlabel('Condition')\n",
    "ax.set_ylabel('Percentage of RDs')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlim([-0.5, num_conditions - 0.5]) \n",
    "\n",
    "\n",
    "ax.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "fig_filename = os.path.join(save_dir, f'{datetime.today().strftime(\"%y%m%d\")}_YL_RDs_bar_percentage.pdf')\n",
    "plt.savefig(fig_filename, dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
